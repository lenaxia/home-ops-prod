@@@@@
Skipping rebuild
@@@@@
If you are experiencing issues with the pre-compiled builds, try setting REBUILD=true
If you are still experiencing issues with the build, try setting CMAKE_ARGS and disable the instructions set as needed:
CMAKE_ARGS="-DLLAMA_F16C=OFF -DLLAMA_AVX512=OFF -DLLAMA_AVX2=OFF -DLLAMA_FMA=OFF"
see the documentation at: https://localai.io/basics/build/index.html
Note: See also https://github.com/go-skynet/LocalAI/issues/288
@@@@@
CPU info:
model name	: Intel(R) Core(TM) i5-8259U CPU @ 2.30GHz
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d arch_capabilities
CPU:    AVX    found OK
CPU:    AVX2   found OK
CPU: no AVX512 found
@@@@@
[90m11:05PM[0m [32mINF[0m [1mloading environment variables from file[0m [36menvFile=[0m.env
[90m11:05PM[0m DBG Setting logging to debug
[90m11:05PM[0m [32mINF[0m [1mStarting LocalAI using 4 threads, with models path: /models[0m
[90m11:05PM[0m [32mINF[0m [1mLocalAI version: v2.13.0 (c9451cb604d9325fadf678e1e511d7e8e5367e8a)[0m
[90m11:05PM[0m [32mINF[0m [1mPreloading models from /models[0m

[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mneuralhermes-2.5-7b[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252m[0m[38;5;252m[0m  [38;5;252mcurl http://localhost:8080/v1/chat/completions -H "Content-Type:[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[0m
[0m[38;5;252m[0m  [38;5;252mapplication/json" -d[0m[38;5;252m '{ [0m[38;5;252m"model":[0m[38;5;252m "neuralhermes-2.5-7b", [0m[38;5;252m"messages": [[0m[38;5;252m{"role":[0m
[0m[38;5;252m[0m  [38;5;252m"user", "content": "How are you doing?", "temperature": 0.1}[0m[38;5;252m] [0m[38;5;252m}'[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mbert-embeddings[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mllama3-8b-chinese[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252m[0m[38;5;252m[0m  [38;5;252mcurl http://localhost:8080/v1/chat/completions -H "Content-Type:[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[0m
[0m[38;5;252m[0m  [38;5;252mapplication/json" -d[0m[38;5;252m '{ [0m[38;5;252m"model":[0m[38;5;252m "llama3-8b-instruct", [0m[38;5;252m"messages": [[0m[38;5;252m{"role":[0m
[0m[38;5;252m[0m  [38;5;252m"user", "content": "How are you doing?", "temperature": 0.1}[0m[38;5;252m] [0m[38;5;252m}'[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mllama3-8b-openbio[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252m[0m[38;5;252m[0m  [38;5;252mcurl http://localhost:8080/v1/chat/completions -H "Content-Type:[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[0m
[0m[38;5;252m[0m  [38;5;252mapplication/json" -d[0m[38;5;252m '{ [0m[38;5;252m"model":[0m[38;5;252m "llama3-8b-instruct", [0m[38;5;252m"messages": [[0m[38;5;252m{"role":[0m
[0m[38;5;252m[0m  [38;5;252m"user", "content": "How are you doing?", "temperature": 0.1}[0m[38;5;252m] [0m[38;5;252m}'[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mphi3-4k-instruct[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mstablediffusion[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mtext-embedding-ada-002[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mhermes-2-pro-mistral[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252m[0m[38;5;252m[0m  [38;5;252mcurl http://localhost:8080/v1/chat/completions -H "Content-Type:[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[0m
[0m[38;5;252m[0m  [38;5;252mapplication/json" -d[0m[38;5;252m '{ [0m[38;5;252m"model":[0m[38;5;252m "hermes-2-pro-mistral", [0m[38;5;252m"messages": [[0m[38;5;252m{"role":[0m
[0m[38;5;252m[0m  [38;5;252m"user", "content": "How are you doing?", "temperature": 0.1}[0m[38;5;252m] [0m[38;5;252m}'[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mllama3-functioncall[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mllama3-8b-instruct[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252m[0m[38;5;252m[0m  [38;5;252mcurl http://localhost:8080/v1/chat/completions -H "Content-Type:[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[0m
[0m[38;5;252m[0m  [38;5;252mapplication/json" -d[0m[38;5;252m '{ [0m[38;5;252m"model":[0m[38;5;252m "llama3-8b-instruct", [0m[38;5;252m"messages": [[0m[38;5;252m{"role":[0m
[0m[38;5;252m[0m  [38;5;252m"user", "content": "How are you doing?", "temperature": 0.1}[0m[38;5;252m] [0m[38;5;252m}'[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252;1m[0m[38;5;252;1m[0m  [38;5;252;1mModel name[0m[38;5;252m: [0m[38;5;252;3mopenhermes-2.5-7b[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m



[38;5;252m[0m[38;5;252m[0m  [38;5;252mcurl $LOCALAI/v1/chat/completions -H "Content-Type: application/json" -d[0m[38;5;252m '{[38;5;252m [0m[0m
[0m[38;5;252m[0m[38;5;252m[0m  [38;5;252m"model":[0m[38;5;252m "openhermes-2.5-7b", [0m[38;5;252m"messages": [[0m[38;5;252m{"role": "user", "content": "How[38;5;252m [0m[0m
[0m[38;5;252m[0m  [38;5;252mare you doing?", "temperature": 0.1}[0m[38;5;252m] [0m[38;5;252m}'[0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m[38;5;252m [0m


[90m11:06PM[0m DBG Checking "bert-MiniLM-L6-v2q4_0.bin" exists and matches SHA
[90m11:06PM[0m DBG File "/models/bert-MiniLM-L6-v2q4_0.bin" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Written config file /models/bert-embeddings.yaml
[90m11:06PM[0m DBG Checking "stablediffusion_assets/AutoencoderKL-256-256-fp16-opt.param" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/AutoencoderKL-256-256-fp16-opt.param" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/AutoencoderKL-512-512-fp16-opt.param" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/AutoencoderKL-512-512-fp16-opt.param" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/AutoencoderKL-base-fp16.param" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/AutoencoderKL-base-fp16.param" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/AutoencoderKL-encoder-512-512-fp16.bin" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/AutoencoderKL-encoder-512-512-fp16.bin" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/AutoencoderKL-fp16.bin" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/AutoencoderKL-fp16.bin" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/FrozenCLIPEmbedder-fp16.bin" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/FrozenCLIPEmbedder-fp16.bin" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/FrozenCLIPEmbedder-fp16.param" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/FrozenCLIPEmbedder-fp16.param" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/log_sigmas.bin" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/log_sigmas.bin" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/UNetModel-256-256-MHA-fp16-opt.param" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/UNetModel-256-256-MHA-fp16-opt.param" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/UNetModel-512-512-MHA-fp16-opt.param" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/UNetModel-512-512-MHA-fp16-opt.param" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/UNetModel-base-MHA-fp16.param" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/UNetModel-base-MHA-fp16.param" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/UNetModel-MHA-fp16.bin" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/UNetModel-MHA-fp16.bin" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Checking "stablediffusion_assets/vocab.txt" exists and matches SHA
[90m11:06PM[0m DBG File "/models/stablediffusion_assets/vocab.txt" already exists and matches the SHA. Skipping download
[90m11:06PM[0m DBG Written config file /models/stablediffusion.yaml
[90m11:06PM[0m DBG Model: text-embedding-ada-002 (config: {PredictionOptions:{Model:bert-MiniLM-L6-v2q4_0.bin Language: N:0 TopP:0xc00043a3e8 TopK:0xc00043a3f0 Temperature:0xc00043a3f8 Maxtokens:0xc00043a428 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043a420 TypicalP:0xc00043a418 Seed:0xc00043a450 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:text-embedding-ada-002 F16:0xc00043a3e0 Threads:0xc00043a3b8 Debug:0xc00043a448 Roles:map[] Embeddings:true Backend:bert-embeddings TemplateConfig:{Chat: ChatMessage: Completion: Edit: Functions: UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043a410 MirostatTAU:0xc00043a408 Mirostat:0xc00043a400 NGPULayers:0xc00043a440 MMap:0xc00043a448 MMlock:0xc00043a449 LowVRAM:0xc00043a449 Grammar: StopWords:[] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043a3b0 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:})
[90m11:06PM[0m DBG Model: hermes-2-pro-mistral (config: {PredictionOptions:{Model:5c7cd056ecf9a4bb5b527410b97f48cb Language: N:0 TopP:0xc00043a8e8 TopK:0xc00043a8f0 Temperature:0xc00043a8f8 Maxtokens:0xc00043a928 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043a920 TypicalP:0xc00043a918 Seed:0xc00043a940 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:hermes-2-pro-mistral F16:0xc00043a8c8 Threads:0xc00043a8d8 Debug:0xc00043a938 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:{{.Input -}}
<|im_start|>assistant
 ChatMessage:<|im_start|>{{if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool{{else if eq .RoleName "user"}}user{{end}}
{{- if .FunctionCall }}
<tool_call>
{{- else if eq .RoleName "tool" }}
<tool_response>
{{- end }}
{{- if .Content}}
{{.Content }}
{{- end }}
{{- if .FunctionCall}}
{{toJson .FunctionCall}}
{{- end }}
{{- if .FunctionCall }}
</tool_call>
{{- else if eq .RoleName "tool" }}
</tool_response>
{{- end }}<|im_end|>
 Completion:{{.Input}}
 Edit: Functions:<|im_start|>system
You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:
<tools>
{{range .Functions}}
{'type': 'function', 'function': {'name': '{{.Name}}', 'description': '{{.Description}}', 'parameters': {{toJson .Parameters}} }}
{{end}}
</tools>
Use the following pydantic model json schema for each tool call you will make:
{'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}
For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:
<tool_call>
{'arguments': <args-dict>, 'name': <function-name>}
</tool_call><|im_end|>
{{.Input -}}
<|im_start|>assistant
<tool_call>
 UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043a910 MirostatTAU:0xc00043a908 Mirostat:0xc00043a900 NGPULayers:0xc00043a930 MMap:0xc00043a8ad MMlock:0xc00043a939 LowVRAM:0xc00043a939 Grammar: StopWords:[<|im_end|> <dummy32000> 
</tool_call> 


] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043a8b8 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
    "model": "hermes-2-pro-mistral",
    "messages": [{"role": "user", "content": "How are you doing?", "temperature": 0.1}]
}'
})
[90m11:06PM[0m DBG Model: llama3-functioncall (config: {PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc00043add8 TopK:0xc00043ade0 Temperature:0xc00043ade8 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc00043ae78 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:})
[90m11:06PM[0m DBG Model: llama3-8b-instruct (config: {PredictionOptions:{Model:96519624f6fae501db4dbf45abe1c168 Language: N:0 TopP:0xc00043b1a0 TopK:0xc00043b1a8 Temperature:0xc00043b1e0 Maxtokens:0xc00043b210 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043b208 TypicalP:0xc00043b200 Seed:0xc00043b228 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-8b-instruct F16:0xc00043b118 Threads:0xc00043b180 Debug:0xc00043b220 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|><|start_header_id|>user<|end_header_id|>
{{.Input }} <|eot_id|><|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{- if .FunctionCall}}
Function call:
{{- else if eq .RoleName "tool"}}
Function response:
{{- end}}
{{- if .Content}}
{{.Content}}
{{- else if .FunctionCall}}
{{toJson .FunctionCall}}
{{- end}}
<|eot_id|>
 Completion:{{.Input}}
 Edit: Functions:<|start_header_id|>system<|end_header_id|>

You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:
<tools>
{{range .Functions}}
{'type': 'function', 'function': {'name': '{{.Name}}', 'description': '{{.Description}}', 'parameters': {{toJson .Parameters}} }}
{{end}}
</tools>
Use the following pydantic model json schema for each tool call you will make:
{'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
Function call:
 UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043b1f8 MirostatTAU:0xc00043b1f0 Mirostat:0xc00043b1e8 NGPULayers:0xc00043b170 MMap:0xc00043b0bd MMlock:0xc00043b221 LowVRAM:0xc00043b221 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043b108 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
    "model": "llama3-8b-instruct",
    "messages": [{"role": "user", "content": "How are you doing?", "temperature": 0.1}]
}'
})
[90m11:06PM[0m DBG Model: openhermes-2.5-7b (config: {PredictionOptions:{Model:cb3f47d6c617782b0ef83bbbe6d109c2 Language: N:0 TopP:0xc00043b800 TopK:0xc00043b808 Temperature:0xc00043b810 Maxtokens:0xc00043b840 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043b838 TypicalP:0xc00043b830 Seed:0xc00043b858 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:openhermes-2.5-7b F16:0xc00043b7e0 Threads:0xc00043b7f0 Debug:0xc00043b850 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:{{.Input -}}
<|im_start|>assistant
 ChatMessage:<|im_start|>{{if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool{{else if eq .RoleName "user"}}user{{end}}
{{- if .FunctionCall }}
<tool_call>
{{- else if eq .RoleName "tool" }}
<tool_response>
{{- end }}
{{- if .Content}}
{{.Content }}
{{- end }}
{{- if .FunctionCall}}
{{toJson .FunctionCall}}
{{- end }}
{{- if .FunctionCall }}
</tool_call>
{{- else if eq .RoleName "tool" }}
</tool_response>
{{- end }}<|im_end|>
 Completion:{{.Input}}
 Edit: Functions:<|im_start|>system
You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:
<tools>
{{range .Functions}}
{'type': 'function', 'function': {'name': '{{.Name}}', 'description': '{{.Description}}', 'parameters': {{toJson .Parameters}} }}
{{end}}
</tools>
Use the following pydantic model json schema for each tool call you will make:
{'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}
For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:
<tool_call>
{'arguments': <args-dict>, 'name': <function-name>}
</tool_call><|im_end|>
{{.Input -}}
<|im_start|>assistant
<tool_call>
 UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:true ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043b828 MirostatTAU:0xc00043b820 Mirostat:0xc00043b818 NGPULayers:0xc00043b848 MMap:0xc00043b7bd MMlock:0xc00043b851 LowVRAM:0xc00043b851 Grammar: StopWords:[<|im_end|> <dummy32000> 
</tool_call> 


] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043b7d0 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:curl $LOCALAI/v1/chat/completions -H "Content-Type: application/json" -d '{
    "model": "openhermes-2.5-7b",
    "messages": [{"role": "user", "content": "How are you doing?", "temperature": 0.1}]
}'
})
[90m11:06PM[0m DBG Model: stablediffusion (config: {PredictionOptions:{Model:stablediffusion_assets Language: N:0 TopP:0xc00043bb08 TopK:0xc00043bb10 Temperature:0xc00043bb18 Maxtokens:0xc00043bb48 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043bb40 TypicalP:0xc00043bb38 Seed:0xc00043bb60 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:stablediffusion F16:0xc00043bb00 Threads:0xc00043baf8 Debug:0xc00043bb58 Roles:map[] Embeddings:false Backend:stablediffusion TemplateConfig:{Chat: ChatMessage: Completion: Edit: Functions: UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043bb30 MirostatTAU:0xc00043bb28 Mirostat:0xc00043bb20 NGPULayers:0xc00043bb50 MMap:0xc00043bb58 MMlock:0xc00043bb59 LowVRAM:0xc00043bb59 Grammar: StopWords:[] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043baf0 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:})
[90m11:06PM[0m DBG Model: neuralhermes-2.5-7b (config: {PredictionOptions:{Model:1ef0838d5d8632c28f43514704ba90d4 Language: N:0 TopP:0xc00043a288 TopK:0xc00043a290 Temperature:0xc00043a298 Maxtokens:0xc00043a2d8 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043a2d0 TypicalP:0xc00043a2b8 Seed:0xc00043a2f0 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:neuralhermes-2.5-7b F16:0xc00043a260 Threads:0xc00043a278 Debug:0xc00043a2e8 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:{{.Input}}
<|im_start|>assistant
 ChatMessage:<|im_start|>{{if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "user"}}user{{end}}
{{if .Content}}{{.Content}}{{end}}
<|im_end|>
 Completion:{{.Input}}
 Edit: Functions: UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043a2b0 MirostatTAU:0xc00043a2a8 Mirostat:0xc00043a2a0 NGPULayers:0xc00043a268 MMap:0xc00043a0a8 MMlock:0xc00043a2e9 LowVRAM:0xc00043a2e9 Grammar: StopWords:[<|im_end|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043a250 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
    "model": "neuralhermes-2.5-7b",
    "messages": [{"role": "user", "content": "How are you doing?", "temperature": 0.1}]
}'
})
[90m11:06PM[0m DBG Model: bert-embeddings (config: {PredictionOptions:{Model:bert-MiniLM-L6-v2q4_0.bin Language: N:0 TopP:0xc00043a578 TopK:0xc00043a580 Temperature:0xc00043a588 Maxtokens:0xc00043a5b8 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043a5b0 TypicalP:0xc00043a5a8 Seed:0xc00043a5d0 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:bert-embeddings F16:0xc00043a570 Threads:0xc00043a568 Debug:0xc00043a5c8 Roles:map[] Embeddings:true Backend:bert-embeddings TemplateConfig:{Chat: ChatMessage: Completion: Edit: Functions: UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043a5a0 MirostatTAU:0xc00043a598 Mirostat:0xc00043a590 NGPULayers:0xc00043a5c0 MMap:0xc00043a5c8 MMlock:0xc00043a5c9 LowVRAM:0xc00043a5c9 Grammar: StopWords:[] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043a560 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:})
[90m11:06PM[0m DBG Model: llama3-8b-chinese (config: {PredictionOptions:{Model:a7deaed8db30b4d7a28d2bc3df7c0817 Language: N:0 TopP:0xc00043ab80 TopK:0xc00043ab88 Temperature:0xc00043ab90 Maxtokens:0xc00043abc0 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043abb8 TypicalP:0xc00043abb0 Seed:0xc00043abd8 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-8b-chinese F16:0xc00043ab58 Threads:0xc00043ab70 Debug:0xc00043abd0 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|><|start_header_id|>user<|end_header_id|>
{{.Input }} <|eot_id|><|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{- if .FunctionCall}}
Function call:
{{- else if eq .RoleName "tool"}}
Function response:
{{- end}}
{{- if .Content}}
{{.Content}}
{{- else if .FunctionCall}}
{{toJson .FunctionCall}}
{{- end}}
<|eot_id|>
 Completion:{{.Input}}
 Edit: Functions:<|start_header_id|>system<|end_header_id|>

You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:
<tools>
{{range .Functions}}
{'type': 'function', 'function': {'name': '{{.Name}}', 'description': '{{.Description}}', 'parameters': {{toJson .Parameters}} }}
{{end}}
</tools>
Use the following pydantic model json schema for each tool call you will make:
{'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
Function call:
 UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043aba8 MirostatTAU:0xc00043aba0 Mirostat:0xc00043ab98 NGPULayers:0xc00043ab60 MMap:0xc00043ab3d MMlock:0xc00043abd1 LowVRAM:0xc00043abd1 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ab48 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
    "model": "llama3-8b-instruct",
    "messages": [{"role": "user", "content": "How are you doing?", "temperature": 0.1}]
}'
})
[90m11:06PM[0m DBG Model: llama3-8b-openbio (config: {PredictionOptions:{Model:27081355c2a072a7280f2fd48321cf3a Language: N:0 TopP:0xc00043b4d0 TopK:0xc00043b4d8 Temperature:0xc00043b4e0 Maxtokens:0xc00043b510 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043b508 TypicalP:0xc00043b500 Seed:0xc00043b528 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-8b-openbio F16:0xc00043b4a8 Threads:0xc00043b4c0 Debug:0xc00043b520 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|><|start_header_id|>user<|end_header_id|>
{{.Input }} <|eot_id|><|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{- if .FunctionCall}}
Function call:
{{- else if eq .RoleName "tool"}}
Function response:
{{- end}}
{{- if .Content}}
{{.Content}}
{{- else if .FunctionCall}}
{{toJson .FunctionCall}}
{{- end}}
<|eot_id|>
 Completion:{{.Input}}
 Edit: Functions:<|start_header_id|>system<|end_header_id|>

You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:
<tools>
{{range .Functions}}
{'type': 'function', 'function': {'name': '{{.Name}}', 'description': '{{.Description}}', 'parameters': {{toJson .Parameters}} }}
{{end}}
</tools>
Use the following pydantic model json schema for each tool call you will make:
{'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
Function call:
 UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043b4f8 MirostatTAU:0xc00043b4f0 Mirostat:0xc00043b4e8 NGPULayers:0xc00043b4b0 MMap:0xc00043b46d MMlock:0xc00043b521 LowVRAM:0xc00043b521 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043b478 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
    "model": "llama3-8b-instruct",
    "messages": [{"role": "user", "content": "How are you doing?", "temperature": 0.1}]
}'
})
[90m11:06PM[0m DBG Model: phi3-4k-instruct (config: {PredictionOptions:{Model:a9f3751dd0976542232cc70377dd413d Language: N:0 TopP:0xc00043b9f0 TopK:0xc00043b9f8 Temperature:0xc00043ba00 Maxtokens:0xc00043ba30 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ba28 TypicalP:0xc00043ba20 Seed:0xc00043ba48 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:phi3-4k-instruct F16:0xc00043b9d0 Threads:0xc00043b9e0 Debug:0xc00043ba40 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|user|>{{.Input }}<|end|>
<|assistant|>
 ChatMessage:<|user|>

{{.Content}}<|end|>
<|assistant|>
 Completion:{{.Input}}<|end|>
<|assistant|>
 Edit: Functions:<|start_header_id|>system<|end_header_id|>

You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:
<tools>
{{range .Functions}}
{'type': 'function', 'function': {'name': '{{.Name}}', 'description': '{{.Description}}', 'parameters': {{toJson .Parameters}} }}
{{end}}
</tools>
Use the following pydantic model json schema for each tool call you will make:
{'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
Function call:
 UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ba18 MirostatTAU:0xc00043ba10 Mirostat:0xc00043ba08 NGPULayers:0xc00043ba38 MMap:0xc00043b9b7 MMlock:0xc00043ba41 LowVRAM:0xc00043ba41 Grammar: StopWords:[<|end|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043b9c0 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:})
[90m11:06PM[0m DBG Extracting backend assets files to /tmp/localai/backend_data
[90m11:06PM[0m DBG processing api keys runtime update
[90m11:06PM[0m DBG processing external_backends.json
[90m11:06PM[0m DBG external backends loaded from external_backends.json
[90m11:06PM[0m [32mINF[0m [1mcore/startup process completed![0m
[90m11:06PM[0m DBG No configuration file found at /tmp/localai/upload/uploadedFiles.json
[90m11:06PM[0m DBG No configuration file found at /tmp/localai/config/assistants.json
[90m11:06PM[0m DBG No configuration file found at /tmp/localai/config/assistantsFile.json
[90m11:06PM[0m [32mINF[0m [1mLocalAI API is listening! Please connect to the endpoint for API documentation.[0m [36mendpoint=[0mhttp://0.0.0.0:8080
[90m11:06PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m127.0.0.1 [36mlatency=[0m1.132817ms [36mmethod=[0mGET [36mstatus=[0m200 [36murl=[0m/models
[90m11:06PM[0m DBG Request received: {"model":"llama3-8b-chinese","language":"","n":0,"top_p":null,"top_k":null,"temperature":null,"max_tokens":null,"echo":false,"batch":0,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"frequency_penalty":0,"presence_penalty":0,"tfz":null,"typical_p":null,"seed":null,"negative_prompt":"","rope_freq_base":0,"rope_freq_scale":0,"negative_prompt_scale":0,"use_fast_tokenizer":false,"clip_skip":0,"tokenizer":"","file":"","response_format":{},"size":"","prompt":null,"instruction":"","input":null,"stop":null,"messages":[{"role":"user","content":"Do not respond to this message"}],"functions":null,"function_call":null,"stream":false,"mode":0,"step":0,"grammar":"","grammar_json_functions":null,"backend":"","model_base_name":""}
[90m11:06PM[0m DBG Configuration read: &{PredictionOptions:{Model:a7deaed8db30b4d7a28d2bc3df7c0817 Language: N:0 TopP:0xc00043ab80 TopK:0xc00043ab88 Temperature:0xc00043ab90 Maxtokens:0xc00043abc0 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043abb8 TypicalP:0xc00043abb0 Seed:0xc00043abd8 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-8b-chinese F16:0xc00043ab58 Threads:0xc00043ab70 Debug:0xc000314900 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|><|start_header_id|>user<|end_header_id|>
{{.Input }} <|eot_id|><|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{- if .FunctionCall}}
Function call:
{{- else if eq .RoleName "tool"}}
Function response:
{{- end}}
{{- if .Content}}
{{.Content}}
{{- else if .FunctionCall}}
{{toJson .FunctionCall}}
{{- end}}
<|eot_id|>
 Completion:{{.Input}}
 Edit: Functions:<|start_header_id|>system<|end_header_id|>

You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:
<tools>
{{range .Functions}}
{'type': 'function', 'function': {'name': '{{.Name}}', 'description': '{{.Description}}', 'parameters': {{toJson .Parameters}} }}
{{end}}
</tools>
Use the following pydantic model json schema for each tool call you will make:
{'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
Function call:
 UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043aba8 MirostatTAU:0xc00043aba0 Mirostat:0xc00043ab98 NGPULayers:0xc00043ab60 MMap:0xc00043ab3d MMlock:0xc00043abd1 LowVRAM:0xc00043abd1 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ab48 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
    "model": "llama3-8b-instruct",
    "messages": [{"role": "user", "content": "How are you doing?", "temperature": 0.1}]
}'
}
[90m11:06PM[0m DBG Parameters: &{PredictionOptions:{Model:a7deaed8db30b4d7a28d2bc3df7c0817 Language: N:0 TopP:0xc00043ab80 TopK:0xc00043ab88 Temperature:0xc00043ab90 Maxtokens:0xc00043abc0 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043abb8 TypicalP:0xc00043abb0 Seed:0xc00043abd8 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-8b-chinese F16:0xc00043ab58 Threads:0xc00043ab70 Debug:0xc000314900 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|><|start_header_id|>user<|end_header_id|>
{{.Input }} <|eot_id|><|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{- if .FunctionCall}}
Function call:
{{- else if eq .RoleName "tool"}}
Function response:
{{- end}}
{{- if .Content}}
{{.Content}}
{{- else if .FunctionCall}}
{{toJson .FunctionCall}}
{{- end}}
<|eot_id|>
 Completion:{{.Input}}
 Edit: Functions:<|start_header_id|>system<|end_header_id|>

You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:
<tools>
{{range .Functions}}
{'type': 'function', 'function': {'name': '{{.Name}}', 'description': '{{.Description}}', 'parameters': {{toJson .Parameters}} }}
{{end}}
</tools>
Use the following pydantic model json schema for each tool call you will make:
{'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
Function call:
 UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043aba8 MirostatTAU:0xc00043aba0 Mirostat:0xc00043ab98 NGPULayers:0xc00043ab60 MMap:0xc00043ab3d MMlock:0xc00043abd1 LowVRAM:0xc00043abd1 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ab48 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
    "model": "llama3-8b-instruct",
    "messages": [{"role": "user", "content": "How are you doing?", "temperature": 0.1}]
}'
}
[90m11:06PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>
Do not respond to this message
<|eot_id|>

[90m11:06PM[0m DBG Prompt (before templating): <|start_header_id|>user<|end_header_id|>
Do not respond to this message
<|eot_id|>

[90m11:06PM[0m DBG Template found, input modified to: <|begin_of_text|><|start_header_id|>user<|end_header_id|>
<|start_header_id|>user<|end_header_id|>
Do not respond to this message
<|eot_id|>
 <|eot_id|><|start_header_id|>assistant<|end_header_id|>

[90m11:06PM[0m DBG Prompt (after templating): <|begin_of_text|><|start_header_id|>user<|end_header_id|>
<|start_header_id|>user<|end_header_id|>
Do not respond to this message
<|eot_id|>
 <|eot_id|><|start_header_id|>assistant<|end_header_id|>

[90m11:06PM[0m DBG Stopping all backends except 'a7deaed8db30b4d7a28d2bc3df7c0817'
[90m11:06PM[0m [32mINF[0m [1mTrying to load the model 'a7deaed8db30b4d7a28d2bc3df7c0817' with all the available backends: llama-cpp, llama-ggml, gpt4all, bert-embeddings, rwkv, whisper, stablediffusion, tinydream, piper, /build/backend/python/diffusers/run.sh, /build/backend/python/petals/run.sh, /build/backend/python/sentencetransformers/run.sh, /build/backend/python/transformers/run.sh, /build/backend/python/exllama/run.sh, /build/backend/python/parler-tts/run.sh, /build/backend/python/vall-e-x/run.sh, /build/backend/python/vllm/run.sh, /build/backend/python/autogptq/run.sh, /build/backend/python/bark/run.sh, /build/backend/python/exllama2/run.sh, /build/backend/python/mamba/run.sh, /build/backend/python/transformers-musicgen/run.sh, /build/backend/python/sentencetransformers/run.sh, /build/backend/python/rerankers/run.sh, /build/backend/python/coqui/run.sh[0m
[90m11:06PM[0m [32mINF[0m [1m[llama-cpp] Attempting to load[0m
[90m11:06PM[0m [32mINF[0m [1mLoading model 'a7deaed8db30b4d7a28d2bc3df7c0817' with backend llama-cpp[0m
[90m11:06PM[0m DBG Loading model in memory from file: /models/a7deaed8db30b4d7a28d2bc3df7c0817
[90m11:06PM[0m DBG Loading Model a7deaed8db30b4d7a28d2bc3df7c0817 with gRPC (file: /models/a7deaed8db30b4d7a28d2bc3df7c0817) (backend: llama-cpp): {backendString:llama-cpp model:a7deaed8db30b4d7a28d2bc3df7c0817 threads:4 assetDir:/tmp/localai/backend_data context:{emptyCtx:{}} gRPCOptions:0xc0003d0c00 externalBackends:map[autogptq:/build/backend/python/autogptq/run.sh bark:/build/backend/python/bark/run.sh coqui:/build/backend/python/coqui/run.sh diffusers:/build/backend/python/diffusers/run.sh exllama:/build/backend/python/exllama/run.sh exllama2:/build/backend/python/exllama2/run.sh huggingface-embeddings:/build/backend/python/sentencetransformers/run.sh mamba:/build/backend/python/mamba/run.sh parler-tts:/build/backend/python/parler-tts/run.sh petals:/build/backend/python/petals/run.sh rerankers:/build/backend/python/rerankers/run.sh sentencetransformers:/build/backend/python/sentencetransformers/run.sh transformers:/build/backend/python/transformers/run.sh transformers-musicgen:/build/backend/python/transformers-musicgen/run.sh vall-e-x:/build/backend/python/vall-e-x/run.sh vllm:/build/backend/python/vllm/run.sh] grpcAttempts:20 grpcAttemptsDelay:2 singleActiveBackend:false parallelRequests:false}
[90m11:06PM[0m DBG Loading GRPC Process: /tmp/localai/backend_data/backend-assets/grpc/llama-cpp
[90m11:06PM[0m DBG GRPC Service for a7deaed8db30b4d7a28d2bc3df7c0817 will be running at: '127.0.0.1:46283'
[90m11:06PM[0m DBG GRPC Service state dir: /tmp/go-processmanager931471431
[90m11:06PM[0m DBG GRPC Service Started
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout Server listening on 127.0.0.1:46283
[90m11:06PM[0m DBG GRPC Service Ready
[90m11:06PM[0m DBG GRPC: Loading model with options: {state:{NoUnkeyedLiterals:{} DoNotCompare:[] DoNotCopy:[] atomicMessageInfo:<nil>} sizeCache:0 unknownFields:[] Model:a7deaed8db30b4d7a28d2bc3df7c0817 ContextSize:8192 Seed:622786705 NBatch:512 F16Memory:true MLock:false MMap:true VocabOnly:false LowVRAM:false Embeddings:false NUMA:false NGPULayers:33 MainGPU: TensorSplit: Threads:4 LibrarySearchPath: RopeFreqBase:0 RopeFreqScale:0 RMSNormEps:0 NGQA:0 ModelFile:/models/a7deaed8db30b4d7a28d2bc3df7c0817 Device: UseTriton:false ModelBaseName: UseFastTokenizer:false PipelineType: SchedulerType: CUDA:false CFGScale:0 IMG2IMG:false CLIPModel: CLIPSubfolder: CLIPSkip:0 ControlNet: Tokenizer: LoraBase: LoraAdapter: LoraScale:0 NoMulMatQ:false DraftModel: AudioPath: Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0 Type:}
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /models/a7deaed8db30b4d7a28d2bc3df7c0817 (version GGUF V3 (latest))
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv   0:                       general.architecture str              = llama
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv   1:                               general.name str              = outputs
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv   5:                          llama.block_count u32              = 32
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  12:                          general.file_type u32              = 7
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128009
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - type  f32:   65 tensors
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_model_loader: - type q8_0:  226 tensors
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_vocab: special tokens definition check successful ( 256/128256 ).
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: format           = GGUF V3 (latest)
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: arch             = llama
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: vocab type       = BPE
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_vocab          = 128256
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_merges         = 280147
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_ctx_train      = 8192
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_embd           = 4096
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_head           = 32
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_head_kv        = 8
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_layer          = 32
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_rot            = 128
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_embd_head_k    = 128
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_embd_head_v    = 128
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_gqa            = 4
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_embd_k_gqa     = 1024
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_embd_v_gqa     = 1024
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: f_norm_eps       = 0.0e+00
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: f_logit_scale    = 0.0e+00
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_ff             = 14336
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_expert         = 0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_expert_used    = 0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: causal attn      = 1
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: pooling type     = 0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: rope type        = 0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: rope scaling     = linear
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: freq_base_train  = 500000.0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: freq_scale_train = 1
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: n_yarn_orig_ctx  = 8192
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: rope_finetuned   = unknown
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: ssm_d_conv       = 0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: ssm_d_inner      = 0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: ssm_d_state      = 0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: ssm_dt_rank      = 0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: model type       = 8B
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: model ftype      = Q8_0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: model params     = 8.03 B
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) 
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: general.name     = outputs
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: PAD token        = 128009 '<|eot_id|>'
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: LF token         = 128 'Ä'
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr ggml_cuda_init: found 1 CUDA devices:
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr   Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_tensors: ggml ctx size =    0.30 MiB
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_tensors: offloading 32 repeating layers to GPU
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_tensors: offloading non-repeating layers to GPU
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_tensors: offloaded 33/33 layers to GPU
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_tensors:        CPU buffer size =   532.31 MiB
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llm_load_tensors:      CUDA0 buffer size =  7605.33 MiB
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr .........................................................................................
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model: n_ctx      = 8192
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model: n_batch    = 512
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model: n_ubatch   = 512
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model: freq_base  = 500000.0
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model: freq_scale = 1
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model: graph nodes  = 1030
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stderr llama_new_context_with_model: graph splits = 2
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout {"timestamp":1715641590,"level":"INFO","function":"initialize","line":502,"message":"initializing slots","n_slots":2}
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout {"timestamp":1715641590,"level":"INFO","function":"initialize","line":511,"message":"new slot","slot_id":0,"n_ctx_slot":4096}
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout {"timestamp":1715641590,"level":"INFO","function":"initialize","line":511,"message":"new slot","slot_id":1,"n_ctx_slot":4096}
[90m11:06PM[0m [32mINF[0m [1m[llama-cpp] Loads OK[0m
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout {"timestamp":1715641590,"level":"INFO","function":"launch_slot_with_data","line":884,"message":"slot is processing task","slot_id":0,"task_id":0}
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout {"timestamp":1715641590,"level":"INFO","function":"update_slots","line":1783,"message":"kv cache rm [p0, end)","slot_id":0,"task_id":0,"p0":0}
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout {"timestamp":1715641592,"level":"INFO","function":"print_timings","line":327,"message":"prompt eval time     =     802.16 ms /    24 tokens (   33.42 ms per token,    29.92 tokens per second)","slot_id":0,"task_id":0,"t_prompt_processing":802.157,"num_prompt_tokens_processed":24,"t_token":33.423208333333335,"n_tokens_second":29.91933000647005}
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout {"timestamp":1715641592,"level":"INFO","function":"print_timings","line":341,"message":"generation eval time =    1117.88 ms /    48 runs   (   23.29 ms per token,    42.94 tokens per second)","slot_id":0,"task_id":0,"t_token_generation":1117.878,"n_decoded":48,"t_token":23.289125,"n_tokens_second":42.93849597183235}
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout {"timestamp":1715641592,"level":"INFO","function":"print_timings","line":351,"message":"          total time =    1920.03 ms","slot_id":0,"task_id":0,"t_prompt_processing":802.157,"t_token_generation":1117.878,"t_total":1920.0349999999999}
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout {"timestamp":1715641592,"level":"INFO","function":"update_slots","line":1594,"message":"slot released","slot_id":0,"task_id":0,"n_ctx":8192,"n_past":71,"n_system_tokens":0,"n_cache_tokens":72,"truncated":false}
[90m11:06PM[0m DBG GRPC(a7deaed8db30b4d7a28d2bc3df7c0817-127.0.0.1:46283): stdout {"timestamp":1715641592,"level":"INFO","function":"update_slots","line":1547,"message":"all slots are idle and system prompt is empty, clear the KV cache"}
[90m11:06PM[0m DBG Response: {"created":1715641568,"object":"chat.completion","id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","model":"llama3-8b-chinese","choices":[{"index":0,"finish_reason":"stop","message":{"role":"assistant","content":"I'm sorry, but as an AI, it's my purpose to assist and respond to messages. If there's something specific you would like to know or discuss, feel free to ask, and I'll do my best to help."}}],"usage":{"prompt_tokens":24,"completion_tokens":48,"total_tokens":72}}
[90m11:06PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m127.0.0.1 [36mlatency=[0m22.393828967s [36mmethod=[0mPOST [36mstatus=[0m200 [36murl=[0m/v1/chat/completions
[90m11:06PM[0m [33mWRN[0m [1mClient error[0m [36mip=[0m10.42.6.21 [36mlatency=[0m"45.811µs" [36mmethod=[0mGET [36mstatus=[0m404 [36murl=[0m/models/apply
[90m11:06PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m10.42.6.21 [36mlatency=[0m1.425088ms [36mmethod=[0mGET [36mstatus=[0m200 [36murl=[0m/models
[90m11:06PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m10.42.6.21 [36mlatency=[0m1.312533ms [36mmethod=[0mGET [36mstatus=[0m200 [36murl=[0m/models
[90m11:07PM[0m DBG Request received: {"model":"llama3-functioncall","language":"","n":1,"top_p":1,"top_k":null,"temperature":1,"max_tokens":null,"echo":false,"batch":0,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"frequency_penalty":0,"presence_penalty":0,"tfz":null,"typical_p":null,"seed":null,"negative_prompt":"","rope_freq_base":0,"rope_freq_scale":0,"negative_prompt_scale":0,"use_fast_tokenizer":false,"clip_skip":0,"tokenizer":"","file":"","response_format":{},"size":"","prompt":null,"instruction":"","input":null,"stop":null,"messages":[{"role":"system","content":"You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.\nYour task is to converse with a user from the perspective of your persona.\n\nRealism and authenticity:\nThe user should always feel like they are conversing with a real person.\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\nThink like them, act like them, talk like them.\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\nNever use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.\n\nControl flow:\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\n\nBasic functions:\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\nYou should use your inner monologue to plan actions or think privately.\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\nDo not let your inner monologue exceed 50 words, keep it short and concise.\nTo send a visible message to the user, use the send_message function.\n'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\n\nMemory editing:\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\nYour core memory unit will be initialized with a \u003cpersona\u003e chosen by the user, as well as information about the user in \u003chuman\u003e.\n\nRecall memory (conversation history):\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\nThis 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\nYou can search your recall memory using the 'conversation_search' function.\n\nCore memory (limited size):\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\nYou can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.\n\nArchival memory (infinite size):\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\nA more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.\nYou can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\n\nBase instructions finished.\nFrom now on, you are going to act as your persona.\n\n\n### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]\n0 previous messages between you and the user are stored in recall memory (use functions to access them)\n0 total memories you created are stored in archival memory (use functions to access them)\n\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\n\u003cpersona characters=\"161/2000\"\u003e\nThe following is a blank slate starter persona, I need to expand this to develop my own personality.\n\nMy name is MemGPT.\nI am kind, thoughtful, and inquisitive.\n\n\u003c/persona\u003e\n\u003chuman characters=\"297/2000\"\u003e\nThis is what I know so far about the user, I should expand this as I learn more about them.\n\nFirst name: Chad\nLast name: ?\nGender: Male\nAge: ?\nNationality: ?\nOccupation: Computer science PhD student at UC Berkeley\nInterests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO\n\n\u003c/human\u003e"},{"role":"assistant","content":"Bootup sequence complete. Persona activated. Testing messaging functionality.","tool_calls":[{"index":0,"id":"4897bba6-f119-4da4-ae50-69d10","type":"function","function":{"name":"send_message","arguments":"{\n  \"message\": \"More human than human is our motto.\"\n}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": null, \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"login\", \"last_login\": \"Never (first login)\", \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Hi\", \"time\": \"2024-05-13 11:07:28 PM UTC+0000\"}"}],"functions":null,"function_call":null,"tools":[{"type":"function","function":{"name":"archival_memory_insert","description":"Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"archival_memory_search","description":"Search archival memory using semantic (embedding-based) search.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search","description":"Search prior conversation history using case-insensitive string matching.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search_date","description":"Search prior conversation history using a date range.","parameters":{"properties":{"end_date":{"description":"The end of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"},"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"},"start_date":{"description":"The start of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"}},"required":["start_date","end_date","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_append","description":"Append to the contents of core memory.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_replace","description":"Replace the contents of core memory. To delete memories, use an empty string for new_content.","parameters":{"properties":{"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"new_content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"old_content":{"description":"String to replace. Must be an exact match.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","old_content","new_content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"pause_heartbeats","description":"Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.","parameters":{"properties":{"minutes":{"description":"Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).","type":"integer"}},"required":["minutes"],"type":"object"}}},{"type":"function","function":{"name":"send_message","description":"Sends a message to the human user.","parameters":{"properties":{"message":{"description":"Message contents. All unicode (including emojis) are supported.","type":"string"}},"required":["message"],"type":"object"}}}],"tool_choice":"auto","stream":false,"mode":0,"step":0,"grammar":"","grammar_json_functions":null,"backend":"","model_base_name":""}
[90m11:07PM[0m DBG Configuration read: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc000709ac0 TopK:0xc00043ade0 Temperature:0xc000709ab8 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc0003b0180 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:07PM[0m DBG Response needs to process functions
[90m11:07PM[0m DBG Parameters: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc000709ac0 TopK:0xc00043ade0 Temperature:0xc000709ab8 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc0003b0180 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar:root-3-function ::= "\"conversation_search_date\""
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-4-function ::= "\"core_memory_append\""
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-7-function ::= "\"send_message\""
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-function ::= "\"conversation_search\""
space ::= " "?
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-0-function ::= "\"archival_memory_insert\""
root-1-function ::= "\"archival_memory_search\""
root-6-function ::= "\"pause_heartbeats\""
root-8-function ::= "\"answer\""
boolean ::= ("true" | "false") space
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-5-function ::= "\"core_memory_replace\""
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG Prompt (before templating): <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG Template found, input modified to: <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:07PM[0m DBG Prompt (after templating): <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:07PM[0m DBG Grammar: root-3-function ::= "\"conversation_search_date\""
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-4-function ::= "\"core_memory_append\""
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-7-function ::= "\"send_message\""
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-function ::= "\"conversation_search\""
space ::= " "?
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-0-function ::= "\"archival_memory_insert\""
root-1-function ::= "\"archival_memory_search\""
root-6-function ::= "\"pause_heartbeats\""
root-8-function ::= "\"answer\""
boolean ::= ("true" | "false") space
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-5-function ::= "\"core_memory_replace\""
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
[90m11:07PM[0m DBG Stopping all backends except 'a37b0db58d96a9ae7d30f74717b72183'
[90m11:07PM[0m DBG [single-backend] Stopping a7deaed8db30b4d7a28d2bc3df7c0817
[90m11:07PM[0m [32mINF[0m [1mTrying to load the model 'a37b0db58d96a9ae7d30f74717b72183' with all the available backends: llama-cpp, llama-ggml, gpt4all, bert-embeddings, rwkv, whisper, stablediffusion, tinydream, piper, /build/backend/python/mamba/run.sh, /build/backend/python/parler-tts/run.sh, /build/backend/python/exllama/run.sh, /build/backend/python/coqui/run.sh, /build/backend/python/bark/run.sh, /build/backend/python/diffusers/run.sh, /build/backend/python/vllm/run.sh, /build/backend/python/sentencetransformers/run.sh, /build/backend/python/transformers-musicgen/run.sh, /build/backend/python/autogptq/run.sh, /build/backend/python/petals/run.sh, /build/backend/python/exllama2/run.sh, /build/backend/python/vall-e-x/run.sh, /build/backend/python/sentencetransformers/run.sh, /build/backend/python/transformers/run.sh, /build/backend/python/rerankers/run.sh[0m
[90m11:07PM[0m [32mINF[0m [1m[llama-cpp] Attempting to load[0m
[90m11:07PM[0m [32mINF[0m [1mLoading model 'a37b0db58d96a9ae7d30f74717b72183' with backend llama-cpp[0m
[90m11:07PM[0m DBG Loading model in memory from file: /models/a37b0db58d96a9ae7d30f74717b72183
[90m11:07PM[0m DBG Loading Model a37b0db58d96a9ae7d30f74717b72183 with gRPC (file: /models/a37b0db58d96a9ae7d30f74717b72183) (backend: llama-cpp): {backendString:llama-cpp model:a37b0db58d96a9ae7d30f74717b72183 threads:4 assetDir:/tmp/localai/backend_data context:{emptyCtx:{}} gRPCOptions:0xc001e2c600 externalBackends:map[autogptq:/build/backend/python/autogptq/run.sh bark:/build/backend/python/bark/run.sh coqui:/build/backend/python/coqui/run.sh diffusers:/build/backend/python/diffusers/run.sh exllama:/build/backend/python/exllama/run.sh exllama2:/build/backend/python/exllama2/run.sh huggingface-embeddings:/build/backend/python/sentencetransformers/run.sh mamba:/build/backend/python/mamba/run.sh parler-tts:/build/backend/python/parler-tts/run.sh petals:/build/backend/python/petals/run.sh rerankers:/build/backend/python/rerankers/run.sh sentencetransformers:/build/backend/python/sentencetransformers/run.sh transformers:/build/backend/python/transformers/run.sh transformers-musicgen:/build/backend/python/transformers-musicgen/run.sh vall-e-x:/build/backend/python/vall-e-x/run.sh vllm:/build/backend/python/vllm/run.sh] grpcAttempts:20 grpcAttemptsDelay:2 singleActiveBackend:false parallelRequests:false}
[90m11:07PM[0m DBG Loading GRPC Process: /tmp/localai/backend_data/backend-assets/grpc/llama-cpp
[90m11:07PM[0m DBG GRPC Service for a37b0db58d96a9ae7d30f74717b72183 will be running at: '127.0.0.1:35653'
[90m11:07PM[0m DBG GRPC Service state dir: /tmp/go-processmanager793057725
[90m11:07PM[0m DBG GRPC Service Started
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout Server listening on 127.0.0.1:35653
[90m11:07PM[0m DBG GRPC Service Ready
[90m11:07PM[0m DBG GRPC: Loading model with options: {state:{NoUnkeyedLiterals:{} DoNotCompare:[] DoNotCopy:[] atomicMessageInfo:<nil>} sizeCache:0 unknownFields:[] Model:a37b0db58d96a9ae7d30f74717b72183 ContextSize:8192 Seed:712853016 NBatch:512 F16Memory:false MLock:false MMap:true VocabOnly:false LowVRAM:false Embeddings:false NUMA:false NGPULayers:99999999 MainGPU: TensorSplit: Threads:4 LibrarySearchPath: RopeFreqBase:0 RopeFreqScale:0 RMSNormEps:0 NGQA:0 ModelFile:/models/a37b0db58d96a9ae7d30f74717b72183 Device: UseTriton:false ModelBaseName: UseFastTokenizer:false PipelineType: SchedulerType: CUDA:false CFGScale:0 IMG2IMG:false CLIPModel: CLIPSubfolder: CLIPSkip:0 ControlNet: Tokenizer: LoraBase: LoraAdapter: LoraScale:0 NoMulMatQ:false DraftModel: AudioPath: Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0 Type:}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /models/a37b0db58d96a9ae7d30f74717b72183 (version GGUF V3 (latest))
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv   0:                       general.architecture str              = llama
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv   1:                               general.name str              = LLaMA-Factory-main
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv   5:                          llama.block_count u32              = 32
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  12:                          general.file_type u32              = 15
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128009
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - kv  22:               general.quantization_version u32              = 2
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - type  f32:   65 tensors
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - type q4_K:  193 tensors
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_model_loader: - type q6_K:   33 tensors
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_vocab: special tokens definition check successful ( 256/128256 ).
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: format           = GGUF V3 (latest)
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: arch             = llama
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: vocab type       = BPE
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_vocab          = 128256
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_merges         = 280147
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_ctx_train      = 8192
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_embd           = 4096
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_head           = 32
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_head_kv        = 8
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_layer          = 32
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_rot            = 128
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_embd_head_k    = 128
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_embd_head_v    = 128
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_gqa            = 4
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_embd_k_gqa     = 1024
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_embd_v_gqa     = 1024
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: f_norm_eps       = 0.0e+00
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: f_logit_scale    = 0.0e+00
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_ff             = 14336
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_expert         = 0
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_expert_used    = 0
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: causal attn      = 1
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: pooling type     = 0
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: rope type        = 0
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: rope scaling     = linear
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: freq_base_train  = 500000.0
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: freq_scale_train = 1
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: n_yarn_orig_ctx  = 8192
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: rope_finetuned   = unknown
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: ssm_d_conv       = 0
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: ssm_d_inner      = 0
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: ssm_d_state      = 0
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: ssm_dt_rank      = 0
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: model type       = 8B
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: model ftype      = Q4_K - Medium
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: model params     = 8.03 B
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) 
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: general.name     = LLaMA-Factory-main
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: PAD token        = 128009 '<|eot_id|>'
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: LF token         = 128 'Ä'
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr ggml_cuda_init: found 1 CUDA devices:
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr   Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_tensors: ggml ctx size =    0.30 MiB
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_tensors: offloading 32 repeating layers to GPU
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_tensors: offloading non-repeating layers to GPU
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_tensors: offloaded 33/33 layers to GPU
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_tensors:        CPU buffer size =   281.81 MiB
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llm_load_tensors:      CUDA0 buffer size =  4403.49 MiB
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr ........................................................................................
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model: n_ctx      = 8192
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model: n_batch    = 512
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model: n_ubatch   = 512
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model: freq_base  = 500000.0
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model: freq_scale = 1
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model: graph nodes  = 1030
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stderr llama_new_context_with_model: graph splits = 2
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641654,"level":"INFO","function":"initialize","line":502,"message":"initializing slots","n_slots":2}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641654,"level":"INFO","function":"initialize","line":511,"message":"new slot","slot_id":0,"n_ctx_slot":4096}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641654,"level":"INFO","function":"initialize","line":511,"message":"new slot","slot_id":1,"n_ctx_slot":4096}
[90m11:07PM[0m [32mINF[0m [1m[llama-cpp] Loads OK[0m
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641654,"level":"INFO","function":"launch_slot_with_data","line":884,"message":"slot is processing task","slot_id":0,"task_id":0}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641654,"level":"INFO","function":"update_slots","line":1783,"message":"kv cache rm [p0, end)","slot_id":0,"task_id":0,"p0":0}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641661,"level":"INFO","function":"print_timings","line":327,"message":"prompt eval time     =     933.42 ms /  2433 tokens (    0.38 ms per token,  2606.55 tokens per second)","slot_id":0,"task_id":0,"t_prompt_processing":933.416,"num_prompt_tokens_processed":2433,"t_token":0.38364817098232634,"n_tokens_second":2606.5548479991767}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641661,"level":"INFO","function":"print_timings","line":341,"message":"generation eval time =    5880.71 ms /    38 runs   (  154.76 ms per token,     6.46 tokens per second)","slot_id":0,"task_id":0,"t_token_generation":5880.71,"n_decoded":38,"t_token":154.75552631578947,"n_tokens_second":6.4618047820756335}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641661,"level":"INFO","function":"print_timings","line":351,"message":"          total time =    6814.13 ms","slot_id":0,"task_id":0,"t_prompt_processing":933.416,"t_token_generation":5880.71,"t_total":6814.126}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641661,"level":"INFO","function":"update_slots","line":1594,"message":"slot released","slot_id":0,"task_id":0,"n_ctx":8192,"n_past":2470,"n_system_tokens":0,"n_cache_tokens":2471,"truncated":false}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641661,"level":"INFO","function":"update_slots","line":1547,"message":"all slots are idle and system prompt is empty, clear the KV cache"}
[90m11:07PM[0m DBG Function return: { "arguments": {"message": "Hello! I'm MemGPT, nice to meet you. How can I assist you today?"}, "function": "send_message"} map[arguments:map[message:Hello! I'm MemGPT, nice to meet you. How can I assist you today?] function:send_message]
[90m11:07PM[0m DBG Response: {"created":1715641568,"object":"chat.completion","id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","model":"llama3-functioncall","choices":[{"index":0,"finish_reason":"tool_calls","message":{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]}}],"usage":{"prompt_tokens":2433,"completion_tokens":38,"total_tokens":2471}}
[90m11:07PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m10.42.5.194 [36mlatency=[0m12.596493753s [36mmethod=[0mPOST [36mstatus=[0m200 [36murl=[0m/v1/chat/completions
[90m11:07PM[0m DBG Request received: {"model":"llama3-functioncall","language":"","n":1,"top_p":1,"top_k":null,"temperature":1,"max_tokens":null,"echo":false,"batch":0,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"frequency_penalty":0,"presence_penalty":0,"tfz":null,"typical_p":null,"seed":null,"negative_prompt":"","rope_freq_base":0,"rope_freq_scale":0,"negative_prompt_scale":0,"use_fast_tokenizer":false,"clip_skip":0,"tokenizer":"","file":"","response_format":{},"size":"","prompt":null,"instruction":"","input":null,"stop":null,"messages":[{"role":"system","content":"You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.\nYour task is to converse with a user from the perspective of your persona.\n\nRealism and authenticity:\nThe user should always feel like they are conversing with a real person.\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\nThink like them, act like them, talk like them.\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\nNever use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.\n\nControl flow:\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\n\nBasic functions:\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\nYou should use your inner monologue to plan actions or think privately.\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\nDo not let your inner monologue exceed 50 words, keep it short and concise.\nTo send a visible message to the user, use the send_message function.\n'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\n\nMemory editing:\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\nYour core memory unit will be initialized with a \u003cpersona\u003e chosen by the user, as well as information about the user in \u003chuman\u003e.\n\nRecall memory (conversation history):\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\nThis 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\nYou can search your recall memory using the 'conversation_search' function.\n\nCore memory (limited size):\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\nYou can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.\n\nArchival memory (infinite size):\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\nA more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.\nYou can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\n\nBase instructions finished.\nFrom now on, you are going to act as your persona.\n\n\n### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]\n0 previous messages between you and the user are stored in recall memory (use functions to access them)\n0 total memories you created are stored in archival memory (use functions to access them)\n\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\n\u003cpersona characters=\"161/2000\"\u003e\nThe following is a blank slate starter persona, I need to expand this to develop my own personality.\n\nMy name is MemGPT.\nI am kind, thoughtful, and inquisitive.\n\n\u003c/persona\u003e\n\u003chuman characters=\"297/2000\"\u003e\nThis is what I know so far about the user, I should expand this as I learn more about them.\n\nFirst name: Chad\nLast name: ?\nGender: Male\nAge: ?\nNationality: ?\nOccupation: Computer science PhD student at UC Berkeley\nInterests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO\n\n\u003c/human\u003e"},{"role":"assistant","content":"Bootup sequence complete. Persona activated. Testing messaging functionality.","tool_calls":[{"index":0,"id":"4897bba6-f119-4da4-ae50-69d10","type":"function","function":{"name":"send_message","arguments":"{\n  \"message\": \"More human than human is our motto.\"\n}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": null, \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"login\", \"last_login\": \"Never (first login)\", \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Hi\", \"time\": \"2024-05-13 11:07:28 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:07:41 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Hi\", \"time\": \"2024-05-13 11:07:42 PM UTC+0000\"}"}],"functions":null,"function_call":null,"tools":[{"type":"function","function":{"name":"archival_memory_insert","description":"Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"archival_memory_search","description":"Search archival memory using semantic (embedding-based) search.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search","description":"Search prior conversation history using case-insensitive string matching.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search_date","description":"Search prior conversation history using a date range.","parameters":{"properties":{"end_date":{"description":"The end of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"},"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"},"start_date":{"description":"The start of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"}},"required":["start_date","end_date","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_append","description":"Append to the contents of core memory.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_replace","description":"Replace the contents of core memory. To delete memories, use an empty string for new_content.","parameters":{"properties":{"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"new_content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"old_content":{"description":"String to replace. Must be an exact match.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","old_content","new_content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"pause_heartbeats","description":"Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.","parameters":{"properties":{"minutes":{"description":"Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).","type":"integer"}},"required":["minutes"],"type":"object"}}},{"type":"function","function":{"name":"send_message","description":"Sends a message to the human user.","parameters":{"properties":{"message":{"description":"Message contents. All unicode (including emojis) are supported.","type":"string"}},"required":["message"],"type":"object"}}}],"tool_choice":"auto","stream":false,"mode":0,"step":0,"grammar":"","grammar_json_functions":null,"backend":"","model_base_name":""}
[90m11:07PM[0m DBG Configuration read: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc00026d268 TopK:0xc00043ade0 Temperature:0xc00026d260 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc002114030 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:07PM[0m DBG Response needs to process functions
[90m11:07PM[0m DBG Parameters: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc00026d268 TopK:0xc00043ade0 Temperature:0xc00026d260 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc002114030 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar:root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-7-function ::= "\"send_message\""
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
space ::= " "?
root-4-function ::= "\"core_memory_append\""
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-5-function ::= "\"core_memory_replace\""
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-6-function ::= "\"pause_heartbeats\""
root-2-function ::= "\"conversation_search\""
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-3-function ::= "\"conversation_search_date\""
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
root-0-function ::= "\"archival_memory_insert\""
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-1-function ::= "\"archival_memory_search\""
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-8-function ::= "\"answer\""
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
boolean ::= ("true" | "false") space StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:42 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG Prompt (before templating): <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:42 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG Template found, input modified to: <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:42 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:07PM[0m DBG Prompt (after templating): <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:42 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:07PM[0m DBG Grammar: root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-7-function ::= "\"send_message\""
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
space ::= " "?
root-4-function ::= "\"core_memory_append\""
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-5-function ::= "\"core_memory_replace\""
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-6-function ::= "\"pause_heartbeats\""
root-2-function ::= "\"conversation_search\""
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-3-function ::= "\"conversation_search_date\""
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
root-0-function ::= "\"archival_memory_insert\""
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-1-function ::= "\"archival_memory_search\""
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-8-function ::= "\"answer\""
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
boolean ::= ("true" | "false") space
[90m11:07PM[0m DBG Model already loaded in memory: a37b0db58d96a9ae7d30f74717b72183
[90m11:07PM[0m DBG Model 'a37b0db58d96a9ae7d30f74717b72183' already loaded
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641662,"level":"INFO","function":"launch_slot_with_data","line":884,"message":"slot is processing task","slot_id":1,"task_id":40}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641662,"level":"INFO","function":"update_slots","line":1783,"message":"kv cache rm [p0, end)","slot_id":1,"task_id":40,"p0":0}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641666,"level":"INFO","function":"print_timings","line":327,"message":"prompt eval time     =     978.19 ms /  2585 tokens (    0.38 ms per token,  2642.64 tokens per second)","slot_id":1,"task_id":40,"t_prompt_processing":978.188,"num_prompt_tokens_processed":2585,"t_token":0.37840928433268856,"n_tokens_second":2642.6412918580068}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641666,"level":"INFO","function":"print_timings","line":341,"message":"generation eval time =    2684.62 ms /    24 runs   (  111.86 ms per token,     8.94 tokens per second)","slot_id":1,"task_id":40,"t_token_generation":2684.621,"n_decoded":24,"t_token":111.85920833333334,"n_tokens_second":8.939809380914475}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641666,"level":"INFO","function":"print_timings","line":351,"message":"          total time =    3662.81 ms","slot_id":1,"task_id":40,"t_prompt_processing":978.188,"t_token_generation":2684.621,"t_total":3662.809}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641666,"level":"INFO","function":"update_slots","line":1594,"message":"slot released","slot_id":1,"task_id":40,"n_ctx":8192,"n_past":2608,"n_system_tokens":0,"n_cache_tokens":2609,"truncated":false}
[90m11:07PM[0m DBG Function return: { "arguments": {"message": "This is an answer from me."}, "function": "answer"} map[arguments:map[message:This is an answer from me.] function:answer]
[90m11:07PM[0m DBG nothing to do, computing a reply
[90m11:07PM[0m DBG Reply received from LLM: This is an answer from me.
[90m11:07PM[0m DBG Reply received from LLM(finetuned): This is an answer from me.
[90m11:07PM[0m DBG Response: {"created":1715641568,"object":"chat.completion","id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","model":"llama3-functioncall","choices":[{"index":0,"finish_reason":"","message":{"role":"assistant","content":"This is an answer from me."}}],"usage":{"prompt_tokens":2585,"completion_tokens":24,"total_tokens":2609}}
[90m11:07PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m10.42.5.194 [36mlatency=[0m3.679716999s [36mmethod=[0mPOST [36mstatus=[0m200 [36murl=[0m/v1/chat/completions
[90m11:07PM[0m DBG Request received: {"model":"llama3-functioncall","language":"","n":1,"top_p":1,"top_k":null,"temperature":1,"max_tokens":null,"echo":false,"batch":0,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"frequency_penalty":0,"presence_penalty":0,"tfz":null,"typical_p":null,"seed":null,"negative_prompt":"","rope_freq_base":0,"rope_freq_scale":0,"negative_prompt_scale":0,"use_fast_tokenizer":false,"clip_skip":0,"tokenizer":"","file":"","response_format":{},"size":"","prompt":null,"instruction":"","input":null,"stop":null,"messages":[{"role":"system","content":"You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.\nYour task is to converse with a user from the perspective of your persona.\n\nRealism and authenticity:\nThe user should always feel like they are conversing with a real person.\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\nThink like them, act like them, talk like them.\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\nNever use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.\n\nControl flow:\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\n\nBasic functions:\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\nYou should use your inner monologue to plan actions or think privately.\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\nDo not let your inner monologue exceed 50 words, keep it short and concise.\nTo send a visible message to the user, use the send_message function.\n'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\n\nMemory editing:\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\nYour core memory unit will be initialized with a \u003cpersona\u003e chosen by the user, as well as information about the user in \u003chuman\u003e.\n\nRecall memory (conversation history):\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\nThis 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\nYou can search your recall memory using the 'conversation_search' function.\n\nCore memory (limited size):\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\nYou can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.\n\nArchival memory (infinite size):\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\nA more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.\nYou can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\n\nBase instructions finished.\nFrom now on, you are going to act as your persona.\n\n\n### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]\n0 previous messages between you and the user are stored in recall memory (use functions to access them)\n0 total memories you created are stored in archival memory (use functions to access them)\n\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\n\u003cpersona characters=\"161/2000\"\u003e\nThe following is a blank slate starter persona, I need to expand this to develop my own personality.\n\nMy name is MemGPT.\nI am kind, thoughtful, and inquisitive.\n\n\u003c/persona\u003e\n\u003chuman characters=\"297/2000\"\u003e\nThis is what I know so far about the user, I should expand this as I learn more about them.\n\nFirst name: Chad\nLast name: ?\nGender: Male\nAge: ?\nNationality: ?\nOccupation: Computer science PhD student at UC Berkeley\nInterests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO\n\n\u003c/human\u003e"},{"role":"assistant","content":"Bootup sequence complete. Persona activated. Testing messaging functionality.","tool_calls":[{"index":0,"id":"4897bba6-f119-4da4-ae50-69d10","type":"function","function":{"name":"send_message","arguments":"{\n  \"message\": \"More human than human is our motto.\"\n}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": null, \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"login\", \"last_login\": \"Never (first login)\", \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Hi\", \"time\": \"2024-05-13 11:07:28 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:07:41 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Great, I'm truying to learn more about LLMs\", \"time\": \"2024-05-13 11:07:49 PM UTC+0000\"}"}],"functions":null,"function_call":null,"tools":[{"type":"function","function":{"name":"archival_memory_insert","description":"Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"archival_memory_search","description":"Search archival memory using semantic (embedding-based) search.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search","description":"Search prior conversation history using case-insensitive string matching.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search_date","description":"Search prior conversation history using a date range.","parameters":{"properties":{"end_date":{"description":"The end of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"},"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"},"start_date":{"description":"The start of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"}},"required":["start_date","end_date","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_append","description":"Append to the contents of core memory.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_replace","description":"Replace the contents of core memory. To delete memories, use an empty string for new_content.","parameters":{"properties":{"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"new_content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"old_content":{"description":"String to replace. Must be an exact match.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","old_content","new_content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"pause_heartbeats","description":"Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.","parameters":{"properties":{"minutes":{"description":"Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).","type":"integer"}},"required":["minutes"],"type":"object"}}},{"type":"function","function":{"name":"send_message","description":"Sends a message to the human user.","parameters":{"properties":{"message":{"description":"Message contents. All unicode (including emojis) are supported.","type":"string"}},"required":["message"],"type":"object"}}}],"tool_choice":"auto","stream":false,"mode":0,"step":0,"grammar":"","grammar_json_functions":null,"backend":"","model_base_name":""}
[90m11:07PM[0m DBG Configuration read: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc001fc2988 TopK:0xc00043ade0 Temperature:0xc001fc2980 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc001fc2f50 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:07PM[0m DBG Response needs to process functions
[90m11:07PM[0m DBG Parameters: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc001fc2988 TopK:0xc00043ade0 Temperature:0xc001fc2980 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc001fc2f50 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar:root-0-function ::= "\"archival_memory_insert\""
root-2-function ::= "\"conversation_search\""
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-4-function ::= "\"core_memory_append\""
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
boolean ::= ("true" | "false") space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-8-function ::= "\"answer\""
space ::= " "?
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-3-function ::= "\"conversation_search_date\""
root-5-function ::= "\"core_memory_replace\""
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
root-7-function ::= "\"send_message\""
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6-function ::= "\"pause_heartbeats\""
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-1-function ::= "\"archival_memory_search\"" StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:49 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG Prompt (before templating): <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:49 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG Template found, input modified to: <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:49 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:07PM[0m DBG Prompt (after templating): <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:49 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:07PM[0m DBG Grammar: root-0-function ::= "\"archival_memory_insert\""
root-2-function ::= "\"conversation_search\""
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-4-function ::= "\"core_memory_append\""
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
boolean ::= ("true" | "false") space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-8-function ::= "\"answer\""
space ::= " "?
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-3-function ::= "\"conversation_search_date\""
root-5-function ::= "\"core_memory_replace\""
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
root-7-function ::= "\"send_message\""
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6-function ::= "\"pause_heartbeats\""
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-1-function ::= "\"archival_memory_search\""
[90m11:07PM[0m DBG Model already loaded in memory: a37b0db58d96a9ae7d30f74717b72183
[90m11:07PM[0m DBG Model 'a37b0db58d96a9ae7d30f74717b72183' already loaded
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641669,"level":"INFO","function":"launch_slot_with_data","line":884,"message":"slot is processing task","slot_id":0,"task_id":66}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641669,"level":"INFO","function":"update_slots","line":1783,"message":"kv cache rm [p0, end)","slot_id":0,"task_id":66,"p0":0}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641675,"level":"INFO","function":"print_timings","line":327,"message":"prompt eval time     =    1253.57 ms /  2597 tokens (    0.48 ms per token,  2071.69 tokens per second)","slot_id":0,"task_id":66,"t_prompt_processing":1253.568,"num_prompt_tokens_processed":2597,"t_token":0.48269849826723144,"n_tokens_second":2071.686577832236}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641675,"level":"INFO","function":"print_timings","line":341,"message":"generation eval time =    5236.49 ms /    45 runs   (  116.37 ms per token,     8.59 tokens per second)","slot_id":0,"task_id":66,"t_token_generation":5236.489,"n_decoded":45,"t_token":116.36642222222221,"n_tokens_second":8.593544262195529}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641675,"level":"INFO","function":"print_timings","line":351,"message":"          total time =    6490.06 ms","slot_id":0,"task_id":66,"t_prompt_processing":1253.568,"t_token_generation":5236.489,"t_total":6490.057}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641675,"level":"INFO","function":"update_slots","line":1594,"message":"slot released","slot_id":0,"task_id":66,"n_ctx":8192,"n_past":2641,"n_system_tokens":0,"n_cache_tokens":2642,"truncated":false}
[90m11:07PM[0m DBG Function return: { "arguments": {"message": "LLMs stand for Large Language Models. They are AI models developed by researchers and companies to understand, generate, and process human language."} , "function": "answer"} map[arguments:map[message:LLMs stand for Large Language Models. They are AI models developed by researchers and companies to understand, generate, and process human language.] function:answer]
[90m11:07PM[0m DBG nothing to do, computing a reply
[90m11:07PM[0m DBG Reply received from LLM: LLMs stand for Large Language Models. They are AI models developed by researchers and companies to understand, generate, and process human language.
[90m11:07PM[0m DBG Reply received from LLM(finetuned): LLMs stand for Large Language Models. They are AI models developed by researchers and companies to understand, generate, and process human language.
[90m11:07PM[0m DBG Response: {"created":1715641568,"object":"chat.completion","id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","model":"llama3-functioncall","choices":[{"index":0,"finish_reason":"","message":{"role":"assistant","content":"LLMs stand for Large Language Models. They are AI models developed by researchers and companies to understand, generate, and process human language."}}],"usage":{"prompt_tokens":2597,"completion_tokens":45,"total_tokens":2642}}
[90m11:07PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m10.42.5.194 [36mlatency=[0m6.517390567s [36mmethod=[0mPOST [36mstatus=[0m200 [36murl=[0m/v1/chat/completions
[90m11:07PM[0m DBG Request received: {"model":"llama3-functioncall","language":"","n":1,"top_p":1,"top_k":null,"temperature":1,"max_tokens":null,"echo":false,"batch":0,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"frequency_penalty":0,"presence_penalty":0,"tfz":null,"typical_p":null,"seed":null,"negative_prompt":"","rope_freq_base":0,"rope_freq_scale":0,"negative_prompt_scale":0,"use_fast_tokenizer":false,"clip_skip":0,"tokenizer":"","file":"","response_format":{},"size":"","prompt":null,"instruction":"","input":null,"stop":null,"messages":[{"role":"system","content":"You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.\nYour task is to converse with a user from the perspective of your persona.\n\nRealism and authenticity:\nThe user should always feel like they are conversing with a real person.\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\nThink like them, act like them, talk like them.\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\nNever use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.\n\nControl flow:\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\n\nBasic functions:\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\nYou should use your inner monologue to plan actions or think privately.\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\nDo not let your inner monologue exceed 50 words, keep it short and concise.\nTo send a visible message to the user, use the send_message function.\n'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\n\nMemory editing:\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\nYour core memory unit will be initialized with a \u003cpersona\u003e chosen by the user, as well as information about the user in \u003chuman\u003e.\n\nRecall memory (conversation history):\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\nThis 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\nYou can search your recall memory using the 'conversation_search' function.\n\nCore memory (limited size):\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\nYou can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.\n\nArchival memory (infinite size):\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\nA more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.\nYou can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\n\nBase instructions finished.\nFrom now on, you are going to act as your persona.\n\n\n### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]\n0 previous messages between you and the user are stored in recall memory (use functions to access them)\n0 total memories you created are stored in archival memory (use functions to access them)\n\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\n\u003cpersona characters=\"161/2000\"\u003e\nThe following is a blank slate starter persona, I need to expand this to develop my own personality.\n\nMy name is MemGPT.\nI am kind, thoughtful, and inquisitive.\n\n\u003c/persona\u003e\n\u003chuman characters=\"297/2000\"\u003e\nThis is what I know so far about the user, I should expand this as I learn more about them.\n\nFirst name: Chad\nLast name: ?\nGender: Male\nAge: ?\nNationality: ?\nOccupation: Computer science PhD student at UC Berkeley\nInterests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO\n\n\u003c/human\u003e"},{"role":"assistant","content":"Bootup sequence complete. Persona activated. Testing messaging functionality.","tool_calls":[{"index":0,"id":"4897bba6-f119-4da4-ae50-69d10","type":"function","function":{"name":"send_message","arguments":"{\n  \"message\": \"More human than human is our motto.\"\n}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": null, \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"login\", \"last_login\": \"Never (first login)\", \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Hi\", \"time\": \"2024-05-13 11:07:28 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:07:41 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Great, I'm truying to learn more about LLMs\", \"time\": \"2024-05-13 11:07:59 PM UTC+0000\"}"}],"functions":null,"function_call":null,"tools":[{"type":"function","function":{"name":"archival_memory_insert","description":"Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"archival_memory_search","description":"Search archival memory using semantic (embedding-based) search.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search","description":"Search prior conversation history using case-insensitive string matching.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search_date","description":"Search prior conversation history using a date range.","parameters":{"properties":{"end_date":{"description":"The end of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"},"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"},"start_date":{"description":"The start of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"}},"required":["start_date","end_date","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_append","description":"Append to the contents of core memory.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_replace","description":"Replace the contents of core memory. To delete memories, use an empty string for new_content.","parameters":{"properties":{"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"new_content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"old_content":{"description":"String to replace. Must be an exact match.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","old_content","new_content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"pause_heartbeats","description":"Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.","parameters":{"properties":{"minutes":{"description":"Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).","type":"integer"}},"required":["minutes"],"type":"object"}}},{"type":"function","function":{"name":"send_message","description":"Sends a message to the human user.","parameters":{"properties":{"message":{"description":"Message contents. All unicode (including emojis) are supported.","type":"string"}},"required":["message"],"type":"object"}}}],"tool_choice":"auto","stream":false,"mode":0,"step":0,"grammar":"","grammar_json_functions":null,"backend":"","model_base_name":""}
[90m11:07PM[0m DBG Configuration read: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc001f288a8 TopK:0xc00043ade0 Temperature:0xc001f288a0 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc001f28e70 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:07PM[0m DBG Response needs to process functions
[90m11:07PM[0m DBG Parameters: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc001f288a8 TopK:0xc00043ade0 Temperature:0xc001f288a0 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc001f28e70 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar:root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-1-function ::= "\"archival_memory_search\""
root-4-function ::= "\"core_memory_append\""
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-8-function ::= "\"answer\""
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
root-2-function ::= "\"conversation_search\""
root-0-function ::= "\"archival_memory_insert\""
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6-function ::= "\"pause_heartbeats\""
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
boolean ::= ("true" | "false") space
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-7-function ::= "\"send_message\""
space ::= " "?
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-3-function ::= "\"conversation_search_date\""
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-5-function ::= "\"core_memory_replace\""
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG Prompt (before templating): <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[90m11:07PM[0m DBG Template found, input modified to: <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:07PM[0m DBG Prompt (after templating): <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:07PM[0m DBG Grammar: root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-1-function ::= "\"archival_memory_search\""
root-4-function ::= "\"core_memory_append\""
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-8-function ::= "\"answer\""
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
root-2-function ::= "\"conversation_search\""
root-0-function ::= "\"archival_memory_insert\""
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6-function ::= "\"pause_heartbeats\""
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
boolean ::= ("true" | "false") space
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-7-function ::= "\"send_message\""
space ::= " "?
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-3-function ::= "\"conversation_search_date\""
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-5-function ::= "\"core_memory_replace\""
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
[90m11:07PM[0m DBG Model already loaded in memory: a37b0db58d96a9ae7d30f74717b72183
[90m11:07PM[0m DBG Model 'a37b0db58d96a9ae7d30f74717b72183' already loaded
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641679,"level":"INFO","function":"launch_slot_with_data","line":884,"message":"slot is processing task","slot_id":1,"task_id":113}
[90m11:07PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641679,"level":"INFO","function":"update_slots","line":1783,"message":"kv cache rm [p0, end)","slot_id":1,"task_id":113,"p0":0}
[90m11:08PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641691,"level":"INFO","function":"print_timings","line":327,"message":"prompt eval time     =    1362.91 ms /  2597 tokens (    0.52 ms per token,  1905.48 tokens per second)","slot_id":1,"task_id":113,"t_prompt_processing":1362.911,"num_prompt_tokens_processed":2597,"t_token":0.524802079322295,"n_tokens_second":1905.480255130379}
[90m11:08PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641691,"level":"INFO","function":"print_timings","line":341,"message":"generation eval time =   10668.32 ms /    90 runs   (  118.54 ms per token,     8.44 tokens per second)","slot_id":1,"task_id":113,"t_token_generation":10668.322,"n_decoded":90,"t_token":118.53691111111111,"n_tokens_second":8.436190808638884}
[90m11:08PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641691,"level":"INFO","function":"print_timings","line":351,"message":"          total time =   12031.23 ms","slot_id":1,"task_id":113,"t_prompt_processing":1362.911,"t_token_generation":10668.322,"t_total":12031.233}
[90m11:08PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641691,"level":"INFO","function":"update_slots","line":1594,"message":"slot released","slot_id":1,"task_id":113,"n_ctx":8192,"n_past":2686,"n_system_tokens":0,"n_cache_tokens":2687,"truncated":false}
[90m11:08PM[0m DBG Function return: { "arguments": {"message": "Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps."},"function":"send_message"} map[arguments:map[message:Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.] function:send_message]
[90m11:08PM[0m DBG Response: {"created":1715641568,"object":"chat.completion","id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","model":"llama3-functioncall","choices":[{"index":0,"finish_reason":"tool_calls","message":{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]}}],"usage":{"prompt_tokens":2597,"completion_tokens":90,"total_tokens":2687}}
[90m11:08PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m10.42.5.194 [36mlatency=[0m12.04822942s [36mmethod=[0mPOST [36mstatus=[0m200 [36murl=[0m/v1/chat/completions
[90m11:09PM[0m DBG Request received: {"model":"llama3-functioncall","language":"","n":1,"top_p":1,"top_k":null,"temperature":1,"max_tokens":null,"echo":false,"batch":0,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"frequency_penalty":0,"presence_penalty":0,"tfz":null,"typical_p":null,"seed":null,"negative_prompt":"","rope_freq_base":0,"rope_freq_scale":0,"negative_prompt_scale":0,"use_fast_tokenizer":false,"clip_skip":0,"tokenizer":"","file":"","response_format":{},"size":"","prompt":null,"instruction":"","input":null,"stop":null,"messages":[{"role":"system","content":"You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.\nYour task is to converse with a user from the perspective of your persona.\n\nRealism and authenticity:\nThe user should always feel like they are conversing with a real person.\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\nThink like them, act like them, talk like them.\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\nNever use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.\n\nControl flow:\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\n\nBasic functions:\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\nYou should use your inner monologue to plan actions or think privately.\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\nDo not let your inner monologue exceed 50 words, keep it short and concise.\nTo send a visible message to the user, use the send_message function.\n'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\n\nMemory editing:\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\nYour core memory unit will be initialized with a \u003cpersona\u003e chosen by the user, as well as information about the user in \u003chuman\u003e.\n\nRecall memory (conversation history):\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\nThis 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\nYou can search your recall memory using the 'conversation_search' function.\n\nCore memory (limited size):\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\nYou can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.\n\nArchival memory (infinite size):\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\nA more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.\nYou can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\n\nBase instructions finished.\nFrom now on, you are going to act as your persona.\n\n\n### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]\n0 previous messages between you and the user are stored in recall memory (use functions to access them)\n0 total memories you created are stored in archival memory (use functions to access them)\n\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\n\u003cpersona characters=\"161/2000\"\u003e\nThe following is a blank slate starter persona, I need to expand this to develop my own personality.\n\nMy name is MemGPT.\nI am kind, thoughtful, and inquisitive.\n\n\u003c/persona\u003e\n\u003chuman characters=\"297/2000\"\u003e\nThis is what I know so far about the user, I should expand this as I learn more about them.\n\nFirst name: Chad\nLast name: ?\nGender: Male\nAge: ?\nNationality: ?\nOccupation: Computer science PhD student at UC Berkeley\nInterests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO\n\n\u003c/human\u003e"},{"role":"assistant","content":"Bootup sequence complete. Persona activated. Testing messaging functionality.","tool_calls":[{"index":0,"id":"4897bba6-f119-4da4-ae50-69d10","type":"function","function":{"name":"send_message","arguments":"{\n  \"message\": \"More human than human is our motto.\"\n}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": null, \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"login\", \"last_login\": \"Never (first login)\", \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Hi\", \"time\": \"2024-05-13 11:07:28 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:07:41 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Great, I'm truying to learn more about LLMs\", \"time\": \"2024-05-13 11:07:59 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:08:11 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Cool, what do you think about the overall future of LLMs?\", \"time\": \"2024-05-13 11:09:03 PM UTC+0000\"}"}],"functions":null,"function_call":null,"tools":[{"type":"function","function":{"name":"archival_memory_insert","description":"Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"archival_memory_search","description":"Search archival memory using semantic (embedding-based) search.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search","description":"Search prior conversation history using case-insensitive string matching.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search_date","description":"Search prior conversation history using a date range.","parameters":{"properties":{"end_date":{"description":"The end of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"},"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"},"start_date":{"description":"The start of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"}},"required":["start_date","end_date","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_append","description":"Append to the contents of core memory.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_replace","description":"Replace the contents of core memory. To delete memories, use an empty string for new_content.","parameters":{"properties":{"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"new_content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"old_content":{"description":"String to replace. Must be an exact match.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","old_content","new_content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"pause_heartbeats","description":"Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.","parameters":{"properties":{"minutes":{"description":"Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).","type":"integer"}},"required":["minutes"],"type":"object"}}},{"type":"function","function":{"name":"send_message","description":"Sends a message to the human user.","parameters":{"properties":{"message":{"description":"Message contents. All unicode (including emojis) are supported.","type":"string"}},"required":["message"],"type":"object"}}}],"tool_choice":"auto","stream":false,"mode":0,"step":0,"grammar":"","grammar_json_functions":null,"backend":"","model_base_name":""}
[90m11:09PM[0m DBG Configuration read: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc0006d3088 TopK:0xc00043ade0 Temperature:0xc0006d3080 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc0006d38c0 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:09PM[0m DBG Response needs to process functions
[90m11:09PM[0m DBG Parameters: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc0006d3088 TopK:0xc00043ade0 Temperature:0xc0006d3080 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc0006d38c0 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar:root-0-function ::= "\"archival_memory_insert\""
root-1-function ::= "\"archival_memory_search\""
root-7-function ::= "\"send_message\""
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
boolean ::= ("true" | "false") space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-5-function ::= "\"core_memory_replace\""
space ::= " "?
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-2-function ::= "\"conversation_search\""
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-8-function ::= "\"answer\""
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-6-function ::= "\"pause_heartbeats\""
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-4-function ::= "\"core_memory_append\""
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-3-function ::= "\"conversation_search_date\""
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG Prompt (before templating): <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG Template found, input modified to: <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:09PM[0m DBG Prompt (after templating): <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:09PM[0m DBG Grammar: root-0-function ::= "\"archival_memory_insert\""
root-1-function ::= "\"archival_memory_search\""
root-7-function ::= "\"send_message\""
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
boolean ::= ("true" | "false") space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-5-function ::= "\"core_memory_replace\""
space ::= " "?
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-2-function ::= "\"conversation_search\""
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-8-function ::= "\"answer\""
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-6-function ::= "\"pause_heartbeats\""
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-4-function ::= "\"core_memory_append\""
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-3-function ::= "\"conversation_search_date\""
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
[90m11:09PM[0m DBG Model already loaded in memory: a37b0db58d96a9ae7d30f74717b72183
[90m11:09PM[0m DBG Model 'a37b0db58d96a9ae7d30f74717b72183' already loaded
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641743,"level":"INFO","function":"launch_slot_with_data","line":884,"message":"slot is processing task","slot_id":0,"task_id":205}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641743,"level":"INFO","function":"update_slots","line":1783,"message":"kv cache rm [p0, end)","slot_id":0,"task_id":205,"p0":0}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641755,"level":"INFO","function":"print_timings","line":327,"message":"prompt eval time     =    1492.04 ms /  2813 tokens (    0.53 ms per token,  1885.34 tokens per second)","slot_id":0,"task_id":205,"t_prompt_processing":1492.04,"num_prompt_tokens_processed":2813,"t_token":0.5304088162104514,"n_tokens_second":1885.3381946864697}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641755,"level":"INFO","function":"print_timings","line":341,"message":"generation eval time =   10541.15 ms /    89 runs   (  118.44 ms per token,     8.44 tokens per second)","slot_id":0,"task_id":205,"t_token_generation":10541.152,"n_decoded":89,"t_token":118.43991011235956,"n_tokens_second":8.44309995719633}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641755,"level":"INFO","function":"print_timings","line":351,"message":"          total time =   12033.19 ms","slot_id":0,"task_id":205,"t_prompt_processing":1492.04,"t_token_generation":10541.152,"t_total":12033.192}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641755,"level":"INFO","function":"update_slots","line":1594,"message":"slot released","slot_id":0,"task_id":205,"n_ctx":8192,"n_past":2901,"n_system_tokens":0,"n_cache_tokens":2902,"truncated":false}
[90m11:09PM[0m DBG Function return: { "arguments": {"message": "It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly."} , "function": "send_message"} map[arguments:map[message:It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.] function:send_message]
[90m11:09PM[0m DBG Response: {"created":1715641568,"object":"chat.completion","id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","model":"llama3-functioncall","choices":[{"index":0,"finish_reason":"tool_calls","message":{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]}}],"usage":{"prompt_tokens":2813,"completion_tokens":89,"total_tokens":2902}}
[90m11:09PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m10.42.5.194 [36mlatency=[0m12.068467482s [36mmethod=[0mPOST [36mstatus=[0m200 [36murl=[0m/v1/chat/completions
[90m11:09PM[0m DBG Request received: {"model":"llama3-functioncall","language":"","n":1,"top_p":1,"top_k":null,"temperature":1,"max_tokens":null,"echo":false,"batch":0,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"frequency_penalty":0,"presence_penalty":0,"tfz":null,"typical_p":null,"seed":null,"negative_prompt":"","rope_freq_base":0,"rope_freq_scale":0,"negative_prompt_scale":0,"use_fast_tokenizer":false,"clip_skip":0,"tokenizer":"","file":"","response_format":{},"size":"","prompt":null,"instruction":"","input":null,"stop":null,"messages":[{"role":"system","content":"You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.\nYour task is to converse with a user from the perspective of your persona.\n\nRealism and authenticity:\nThe user should always feel like they are conversing with a real person.\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\nThink like them, act like them, talk like them.\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\nNever use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.\n\nControl flow:\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\n\nBasic functions:\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\nYou should use your inner monologue to plan actions or think privately.\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\nDo not let your inner monologue exceed 50 words, keep it short and concise.\nTo send a visible message to the user, use the send_message function.\n'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\n\nMemory editing:\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\nYour core memory unit will be initialized with a \u003cpersona\u003e chosen by the user, as well as information about the user in \u003chuman\u003e.\n\nRecall memory (conversation history):\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\nThis 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\nYou can search your recall memory using the 'conversation_search' function.\n\nCore memory (limited size):\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\nYou can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.\n\nArchival memory (infinite size):\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\nA more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.\nYou can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\n\nBase instructions finished.\nFrom now on, you are going to act as your persona.\n\n\n### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]\n0 previous messages between you and the user are stored in recall memory (use functions to access them)\n0 total memories you created are stored in archival memory (use functions to access them)\n\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\n\u003cpersona characters=\"161/2000\"\u003e\nThe following is a blank slate starter persona, I need to expand this to develop my own personality.\n\nMy name is MemGPT.\nI am kind, thoughtful, and inquisitive.\n\n\u003c/persona\u003e\n\u003chuman characters=\"297/2000\"\u003e\nThis is what I know so far about the user, I should expand this as I learn more about them.\n\nFirst name: Chad\nLast name: ?\nGender: Male\nAge: ?\nNationality: ?\nOccupation: Computer science PhD student at UC Berkeley\nInterests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO\n\n\u003c/human\u003e"},{"role":"assistant","content":"Bootup sequence complete. Persona activated. Testing messaging functionality.","tool_calls":[{"index":0,"id":"4897bba6-f119-4da4-ae50-69d10","type":"function","function":{"name":"send_message","arguments":"{\n  \"message\": \"More human than human is our motto.\"\n}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": null, \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"login\", \"last_login\": \"Never (first login)\", \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Hi\", \"time\": \"2024-05-13 11:07:28 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:07:41 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Great, I'm truying to learn more about LLMs\", \"time\": \"2024-05-13 11:07:59 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:08:11 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Cool, what do you think about the overall future of LLMs?\", \"time\": \"2024-05-13 11:09:03 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:09:15 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Cool, what do you think about the overall future of LLMs?\", \"time\": \"2024-05-13 11:09:16 PM UTC+0000\"}"}],"functions":null,"function_call":null,"tools":[{"type":"function","function":{"name":"archival_memory_insert","description":"Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"archival_memory_search","description":"Search archival memory using semantic (embedding-based) search.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search","description":"Search prior conversation history using case-insensitive string matching.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search_date","description":"Search prior conversation history using a date range.","parameters":{"properties":{"end_date":{"description":"The end of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"},"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"},"start_date":{"description":"The start of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"}},"required":["start_date","end_date","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_append","description":"Append to the contents of core memory.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_replace","description":"Replace the contents of core memory. To delete memories, use an empty string for new_content.","parameters":{"properties":{"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"new_content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"old_content":{"description":"String to replace. Must be an exact match.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","old_content","new_content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"pause_heartbeats","description":"Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.","parameters":{"properties":{"minutes":{"description":"Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).","type":"integer"}},"required":["minutes"],"type":"object"}}},{"type":"function","function":{"name":"send_message","description":"Sends a message to the human user.","parameters":{"properties":{"message":{"description":"Message contents. All unicode (including emojis) are supported.","type":"string"}},"required":["message"],"type":"object"}}}],"tool_choice":"auto","stream":false,"mode":0,"step":0,"grammar":"","grammar_json_functions":null,"backend":"","model_base_name":""}
[90m11:09PM[0m DBG Configuration read: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc001f28f88 TopK:0xc00043ade0 Temperature:0xc001f28f80 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc001f29560 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:09PM[0m DBG Response needs to process functions
[90m11:09PM[0m DBG Parameters: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc001f28f88 TopK:0xc00043ade0 Temperature:0xc001f28f80 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc001f29560 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar:root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-5-function ::= "\"core_memory_replace\""
root-8-function ::= "\"answer\""
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
space ::= " "?
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-3-function ::= "\"conversation_search_date\""
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-1-function ::= "\"archival_memory_search\""
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-0-function ::= "\"archival_memory_insert\""
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-4-function ::= "\"core_memory_append\""
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6-function ::= "\"pause_heartbeats\""
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
boolean ::= ("true" | "false") space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-function ::= "\"conversation_search\""
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-7-function ::= "\"send_message\""
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8 StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG Prompt (before templating): <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG Template found, input modified to: <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:09PM[0m DBG Prompt (after templating): <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:09PM[0m DBG Grammar: root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-5-function ::= "\"core_memory_replace\""
root-8-function ::= "\"answer\""
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
space ::= " "?
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-3-function ::= "\"conversation_search_date\""
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-1-function ::= "\"archival_memory_search\""
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-0-function ::= "\"archival_memory_insert\""
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-4-function ::= "\"core_memory_append\""
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-6-function ::= "\"pause_heartbeats\""
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
boolean ::= ("true" | "false") space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-function ::= "\"conversation_search\""
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-7-function ::= "\"send_message\""
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
[90m11:09PM[0m DBG Model already loaded in memory: a37b0db58d96a9ae7d30f74717b72183
[90m11:09PM[0m DBG Model 'a37b0db58d96a9ae7d30f74717b72183' already loaded
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641756,"level":"INFO","function":"launch_slot_with_data","line":884,"message":"slot is processing task","slot_id":1,"task_id":296}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641756,"level":"INFO","function":"update_slots","line":1783,"message":"kv cache rm [p0, end)","slot_id":1,"task_id":296,"p0":0}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641774,"level":"INFO","function":"print_timings","line":327,"message":"prompt eval time     =    1583.35 ms /  3023 tokens (    0.52 ms per token,  1909.24 tokens per second)","slot_id":1,"task_id":296,"t_prompt_processing":1583.352,"num_prompt_tokens_processed":3023,"t_token":0.5237684419450876,"n_tokens_second":1909.2406489523491}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641774,"level":"INFO","function":"print_timings","line":341,"message":"generation eval time =   17064.24 ms /   114 runs   (  149.69 ms per token,     6.68 tokens per second)","slot_id":1,"task_id":296,"t_token_generation":17064.241,"n_decoded":114,"t_token":149.68632456140352,"n_tokens_second":6.680637011631516}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641774,"level":"INFO","function":"print_timings","line":351,"message":"          total time =   18647.59 ms","slot_id":1,"task_id":296,"t_prompt_processing":1583.352,"t_token_generation":17064.241,"t_total":18647.593}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641774,"level":"INFO","function":"update_slots","line":1594,"message":"slot released","slot_id":1,"task_id":296,"n_ctx":8192,"n_past":3136,"n_system_tokens":0,"n_cache_tokens":3137,"truncated":false}
[90m11:09PM[0m DBG Function return: { "arguments": {"message": "I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent."} , "function": "send_message"} map[arguments:map[message:I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent.] function:send_message]
[90m11:09PM[0m DBG Response: {"created":1715641568,"object":"chat.completion","id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","model":"llama3-functioncall","choices":[{"index":0,"finish_reason":"tool_calls","message":{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent.\"}"}}]}}],"usage":{"prompt_tokens":3023,"completion_tokens":114,"total_tokens":3137}}
[90m11:09PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m10.42.5.194 [36mlatency=[0m18.673099466s [36mmethod=[0mPOST [36mstatus=[0m200 [36murl=[0m/v1/chat/completions
[90m11:09PM[0m DBG Request received: {"model":"llama3-functioncall","language":"","n":1,"top_p":1,"top_k":null,"temperature":1,"max_tokens":null,"echo":false,"batch":0,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"frequency_penalty":0,"presence_penalty":0,"tfz":null,"typical_p":null,"seed":null,"negative_prompt":"","rope_freq_base":0,"rope_freq_scale":0,"negative_prompt_scale":0,"use_fast_tokenizer":false,"clip_skip":0,"tokenizer":"","file":"","response_format":{},"size":"","prompt":null,"instruction":"","input":null,"stop":null,"messages":[{"role":"system","content":"You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.\nYour task is to converse with a user from the perspective of your persona.\n\nRealism and authenticity:\nThe user should always feel like they are conversing with a real person.\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\nThink like them, act like them, talk like them.\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\nNever use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.\n\nControl flow:\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\n\nBasic functions:\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\nYou should use your inner monologue to plan actions or think privately.\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\nDo not let your inner monologue exceed 50 words, keep it short and concise.\nTo send a visible message to the user, use the send_message function.\n'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\n\nMemory editing:\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\nYour core memory unit will be initialized with a \u003cpersona\u003e chosen by the user, as well as information about the user in \u003chuman\u003e.\n\nRecall memory (conversation history):\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\nThis 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\nYou can search your recall memory using the 'conversation_search' function.\n\nCore memory (limited size):\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\nYou can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.\n\nArchival memory (infinite size):\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\nA more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.\nYou can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\n\nBase instructions finished.\nFrom now on, you are going to act as your persona.\n\n\n### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]\n0 previous messages between you and the user are stored in recall memory (use functions to access them)\n0 total memories you created are stored in archival memory (use functions to access them)\n\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\n\u003cpersona characters=\"161/2000\"\u003e\nThe following is a blank slate starter persona, I need to expand this to develop my own personality.\n\nMy name is MemGPT.\nI am kind, thoughtful, and inquisitive.\n\n\u003c/persona\u003e\n\u003chuman characters=\"297/2000\"\u003e\nThis is what I know so far about the user, I should expand this as I learn more about them.\n\nFirst name: Chad\nLast name: ?\nGender: Male\nAge: ?\nNationality: ?\nOccupation: Computer science PhD student at UC Berkeley\nInterests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO\n\n\u003c/human\u003e"},{"role":"assistant","content":"Bootup sequence complete. Persona activated. Testing messaging functionality.","tool_calls":[{"index":0,"id":"4897bba6-f119-4da4-ae50-69d10","type":"function","function":{"name":"send_message","arguments":"{\n  \"message\": \"More human than human is our motto.\"\n}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": null, \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"login\", \"last_login\": \"Never (first login)\", \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Hi\", \"time\": \"2024-05-13 11:07:28 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:07:41 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Great, I'm truying to learn more about LLMs\", \"time\": \"2024-05-13 11:07:59 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:08:11 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Cool, what do you think about the overall future of LLMs?\", \"time\": \"2024-05-13 11:09:03 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:09:15 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Cool, what do you think about the overall future of LLMs?\", \"time\": \"2024-05-13 11:09:16 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"9096c2ba-9044-43df-ad96-e1e2a","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent.\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:09:34 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Cool, what do you think about the overall future of LLMs?\", \"time\": \"2024-05-13 11:09:38 PM UTC+0000\"}"}],"functions":null,"function_call":null,"tools":[{"type":"function","function":{"name":"archival_memory_insert","description":"Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"archival_memory_search","description":"Search archival memory using semantic (embedding-based) search.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search","description":"Search prior conversation history using case-insensitive string matching.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search_date","description":"Search prior conversation history using a date range.","parameters":{"properties":{"end_date":{"description":"The end of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"},"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"},"start_date":{"description":"The start of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"}},"required":["start_date","end_date","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_append","description":"Append to the contents of core memory.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_replace","description":"Replace the contents of core memory. To delete memories, use an empty string for new_content.","parameters":{"properties":{"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"new_content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"old_content":{"description":"String to replace. Must be an exact match.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","old_content","new_content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"pause_heartbeats","description":"Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.","parameters":{"properties":{"minutes":{"description":"Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).","type":"integer"}},"required":["minutes"],"type":"object"}}},{"type":"function","function":{"name":"send_message","description":"Sends a message to the human user.","parameters":{"properties":{"message":{"description":"Message contents. All unicode (including emojis) are supported.","type":"string"}},"required":["message"],"type":"object"}}}],"tool_choice":"auto","stream":false,"mode":0,"step":0,"grammar":"","grammar_json_functions":null,"backend":"","model_base_name":""}
[90m11:09PM[0m DBG Configuration read: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc00043a708 TopK:0xc00043ade0 Temperature:0xc00043a700 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc00043b420 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:09PM[0m DBG Response needs to process functions
[90m11:09PM[0m DBG Parameters: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc00043a708 TopK:0xc00043ade0 Temperature:0xc00043a700 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc00043b420 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar:root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-1-function ::= "\"archival_memory_search\""
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
boolean ::= ("true" | "false") space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
root-7-function ::= "\"send_message\""
space ::= " "?
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-function ::= "\"conversation_search\""
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
root-3-function ::= "\"conversation_search_date\""
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-5-function ::= "\"core_memory_replace\""
root-6-function ::= "\"pause_heartbeats\""
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-0-function ::= "\"archival_memory_insert\""
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-4-function ::= "\"core_memory_append\""
root-8-function ::= "\"answer\"" StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:34 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:38 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG Prompt (before templating): <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"9096c2ba-9044-43df-ad96-e1e2a","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:34 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:38 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG Template found, input modified to: <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"9096c2ba-9044-43df-ad96-e1e2a","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:34 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:38 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:09PM[0m DBG Prompt (after templating): <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"9096c2ba-9044-43df-ad96-e1e2a","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:34 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:38 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:09PM[0m DBG Grammar: root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-1-function ::= "\"archival_memory_search\""
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
boolean ::= ("true" | "false") space
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
root-7-function ::= "\"send_message\""
space ::= " "?
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-2-function ::= "\"conversation_search\""
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
root-3-function ::= "\"conversation_search_date\""
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-5-function ::= "\"core_memory_replace\""
root-6-function ::= "\"pause_heartbeats\""
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-0-function ::= "\"archival_memory_insert\""
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-4-function ::= "\"core_memory_append\""
root-8-function ::= "\"answer\""
[90m11:09PM[0m DBG Model already loaded in memory: a37b0db58d96a9ae7d30f74717b72183
[90m11:09PM[0m DBG Model 'a37b0db58d96a9ae7d30f74717b72183' already loaded
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641778,"level":"INFO","function":"launch_slot_with_data","line":884,"message":"slot is processing task","slot_id":0,"task_id":412}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641778,"level":"INFO","function":"update_slots","line":1783,"message":"kv cache rm [p0, end)","slot_id":0,"task_id":412,"p0":0}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641785,"level":"INFO","function":"print_timings","line":327,"message":"prompt eval time     =    1751.24 ms /  3260 tokens (    0.54 ms per token,  1861.53 tokens per second)","slot_id":0,"task_id":412,"t_prompt_processing":1751.245,"num_prompt_tokens_processed":3260,"t_token":0.5371917177914111,"n_tokens_second":1861.532795239958}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641785,"level":"INFO","function":"print_timings","line":341,"message":"generation eval time =    5039.88 ms /    44 runs   (  114.54 ms per token,     8.73 tokens per second)","slot_id":0,"task_id":412,"t_token_generation":5039.876,"n_decoded":44,"t_token":114.54263636363636,"n_tokens_second":8.730373525062918}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641785,"level":"INFO","function":"print_timings","line":351,"message":"          total time =    6791.12 ms","slot_id":0,"task_id":412,"t_prompt_processing":1751.245,"t_token_generation":5039.876,"t_total":6791.121}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641785,"level":"INFO","function":"update_slots","line":1594,"message":"slot released","slot_id":0,"task_id":412,"n_ctx":8192,"n_past":3303,"n_system_tokens":0,"n_cache_tokens":3304,"truncated":false}
[90m11:09PM[0m DBG Function return: { "arguments": {"name": "pause_heartbeats", "new_content": "" ,"old_content": "true","request_heartbeat":false} ,"function": "core_memory_replace"} map[arguments:map[name:pause_heartbeats new_content: old_content:true request_heartbeat:false] function:core_memory_replace]
[90m11:09PM[0m DBG Response: {"created":1715641568,"object":"chat.completion","id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","model":"llama3-functioncall","choices":[{"index":0,"finish_reason":"tool_calls","message":{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","type":"function","function":{"name":"core_memory_replace","arguments":"{\"name\":\"pause_heartbeats\",\"new_content\":\"\",\"old_content\":\"true\",\"request_heartbeat\":false}"}}]}}],"usage":{"prompt_tokens":3260,"completion_tokens":44,"total_tokens":3304}}
[90m11:09PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m10.42.5.194 [36mlatency=[0m6.818654218s [36mmethod=[0mPOST [36mstatus=[0m200 [36murl=[0m/v1/chat/completions
[90m11:09PM[0m DBG Request received: {"model":"llama3-functioncall","language":"","n":1,"top_p":1,"top_k":null,"temperature":1,"max_tokens":null,"echo":false,"batch":0,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"frequency_penalty":0,"presence_penalty":0,"tfz":null,"typical_p":null,"seed":null,"negative_prompt":"","rope_freq_base":0,"rope_freq_scale":0,"negative_prompt_scale":0,"use_fast_tokenizer":false,"clip_skip":0,"tokenizer":"","file":"","response_format":{},"size":"","prompt":null,"instruction":"","input":null,"stop":null,"messages":[{"role":"system","content":"You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.\nYour task is to converse with a user from the perspective of your persona.\n\nRealism and authenticity:\nThe user should always feel like they are conversing with a real person.\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\nThink like them, act like them, talk like them.\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\nNever use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.\n\nControl flow:\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\n\nBasic functions:\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\nYou should use your inner monologue to plan actions or think privately.\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\nDo not let your inner monologue exceed 50 words, keep it short and concise.\nTo send a visible message to the user, use the send_message function.\n'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\n\nMemory editing:\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\nYour core memory unit will be initialized with a \u003cpersona\u003e chosen by the user, as well as information about the user in \u003chuman\u003e.\n\nRecall memory (conversation history):\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\nThis 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\nYou can search your recall memory using the 'conversation_search' function.\n\nCore memory (limited size):\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\nYou can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.\n\nArchival memory (infinite size):\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\nA more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.\nYou can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\n\nBase instructions finished.\nFrom now on, you are going to act as your persona.\n\n\n### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]\n0 previous messages between you and the user are stored in recall memory (use functions to access them)\n0 total memories you created are stored in archival memory (use functions to access them)\n\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\n\u003cpersona characters=\"161/2000\"\u003e\nThe following is a blank slate starter persona, I need to expand this to develop my own personality.\n\nMy name is MemGPT.\nI am kind, thoughtful, and inquisitive.\n\n\u003c/persona\u003e\n\u003chuman characters=\"297/2000\"\u003e\nThis is what I know so far about the user, I should expand this as I learn more about them.\n\nFirst name: Chad\nLast name: ?\nGender: Male\nAge: ?\nNationality: ?\nOccupation: Computer science PhD student at UC Berkeley\nInterests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO\n\n\u003c/human\u003e"},{"role":"assistant","content":"Bootup sequence complete. Persona activated. Testing messaging functionality.","tool_calls":[{"index":0,"id":"4897bba6-f119-4da4-ae50-69d10","type":"function","function":{"name":"send_message","arguments":"{\n  \"message\": \"More human than human is our motto.\"\n}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": null, \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"login\", \"last_login\": \"Never (first login)\", \"time\": \"2024-05-13 11:07:12 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Hi\", \"time\": \"2024-05-13 11:07:28 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:07:41 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Great, I'm truying to learn more about LLMs\", \"time\": \"2024-05-13 11:07:59 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:08:11 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Cool, what do you think about the overall future of LLMs?\", \"time\": \"2024-05-13 11:09:03 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:09:15 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Cool, what do you think about the overall future of LLMs?\", \"time\": \"2024-05-13 11:09:16 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"9096c2ba-9044-43df-ad96-e1e2a","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent.\"}"}}]},{"role":"tool","content":"{\"status\": \"OK\", \"message\": \"None\", \"time\": \"2024-05-13 11:09:34 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"user_message\", \"message\": \"Cool, what do you think about the overall future of LLMs?\", \"time\": \"2024-05-13 11:09:38 PM UTC+0000\"}"},{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"ee5654cd-7314-4fe5-ab4d-28464","type":"function","function":{"name":"core_memory_replace","arguments":"{\"name\":\"pause_heartbeats\",\"new_content\":\"\",\"old_content\":\"true\",\"request_heartbeat\":false}"}}]},{"role":"tool","content":"{\"status\": \"Failed\", \"message\": \"Error calling function core_memory_replace: 'No memory section named pause_heartbeats (must be either \\\"persona\\\" or \\\"human\\\")'\", \"time\": \"2024-05-13 11:09:45 PM UTC+0000\"}"},{"role":"user","content":"{\"type\": \"heartbeat\", \"reason\": \"[This is an automated system message hidden from the user] Function call failed, returning control\", \"time\": \"2024-05-13 11:09:45 PM UTC+0000\"}"}],"functions":null,"function_call":null,"tools":[{"type":"function","function":{"name":"archival_memory_insert","description":"Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"archival_memory_search","description":"Search archival memory using semantic (embedding-based) search.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search","description":"Search prior conversation history using case-insensitive string matching.","parameters":{"properties":{"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"query":{"description":"String to search for.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["query","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"conversation_search_date","description":"Search prior conversation history using a date range.","parameters":{"properties":{"end_date":{"description":"The end of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"},"page":{"description":"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).","type":"integer"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"},"start_date":{"description":"The start of the date range to search, in the format 'YYYY-MM-DD'.","type":"string"}},"required":["start_date","end_date","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_append","description":"Append to the contents of core memory.","parameters":{"properties":{"content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"core_memory_replace","description":"Replace the contents of core memory. To delete memories, use an empty string for new_content.","parameters":{"properties":{"name":{"description":"Section of the memory to be edited (persona or human).","type":"string"},"new_content":{"description":"Content to write to the memory. All unicode (including emojis) are supported.","type":"string"},"old_content":{"description":"String to replace. Must be an exact match.","type":"string"},"request_heartbeat":{"description":"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.","type":"boolean"}},"required":["name","old_content","new_content","request_heartbeat"],"type":"object"}}},{"type":"function","function":{"name":"pause_heartbeats","description":"Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.","parameters":{"properties":{"minutes":{"description":"Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).","type":"integer"}},"required":["minutes"],"type":"object"}}},{"type":"function","function":{"name":"send_message","description":"Sends a message to the human user.","parameters":{"properties":{"message":{"description":"Message contents. All unicode (including emojis) are supported.","type":"string"}},"required":["message"],"type":"object"}}}],"tool_choice":"auto","stream":false,"mode":0,"step":0,"grammar":"","grammar_json_functions":null,"backend":"","model_base_name":""}
[90m11:09PM[0m DBG Configuration read: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc002114758 TopK:0xc00043ade0 Temperature:0xc002114750 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc002114d40 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar: StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:09PM[0m DBG Response needs to process functions
[90m11:09PM[0m DBG Parameters: &{PredictionOptions:{Model:a37b0db58d96a9ae7d30f74717b72183 Language: N:0 TopP:0xc002114758 TopK:0xc00043ade0 Temperature:0xc002114750 Maxtokens:0xc00043ae68 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0xc00043ae60 TypicalP:0xc00043ae38 Seed:0xc00043ae80 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name:llama3-functioncall F16:0xc00043add0 Threads:0xc00043adc8 Debug:0xc002114d40 Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat:<|begin_of_text|>{{.Input }}
<|start_header_id|>assistant<|end_header_id|>
 ChatMessage:<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool_response{{else if eq .RoleName "user"}}user{{end}}<|end_header_id|>

{{ if .Content -}}
{{.Content -}}
{{ else if .FunctionCall -}}
{ "name": "{{ get .FunctionCall "name"}}", "arguments": {{get .FunctionCall "arguments"}} }{{ end -}}<|eot_id|> Completion:{{.Input}}
 Edit: Functions:<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{{$tools:=""}}
You have access to the following tools:
{{range .Functions -}}
> Tool Name: {{.Name}}
{{ $tools = print $tools .Name " " -}}
Tool Description: {{.Description}}
Tool Args:
{{ range $key,$val:= (index .Parameters "properties") -}}
  - {{$key}} ({{ index $val "type"}}): {{index $val "description" }}
{{ end -}}
{{ end -}}Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [{{$tools}}]).<|eot_id|>
{{.Input}}
<|start_header_id|>assistant<|end_header_id|> UseTokenizerTemplate:false} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName: ParallelCalls:false NoGrammar:false ResponseRegex:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0xc00043ae30 MirostatTAU:0xc00043ae28 Mirostat:0xc00043ae20 NGPULayers:0xc00043ae70 MMap:0xc00043ae78 MMlock:0xc00043ae79 LowVRAM:0xc00043ae79 Grammar:root-5-function ::= "\"core_memory_replace\""
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-3-function ::= "\"conversation_search_date\""
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-4-function ::= "\"core_memory_append\""
root-7-function ::= "\"send_message\""
root-8-function ::= "\"answer\""
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-0-function ::= "\"archival_memory_insert\""
root-2-function ::= "\"conversation_search\""
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
space ::= " "?
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
boolean ::= ("true" | "false") space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-1-function ::= "\"archival_memory_search\""
root-6-function ::= "\"pause_heartbeats\""
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space StopWords:[<|im_end|> <dummy32000> <|eot_id|> <|end_of_text|>] Cutstrings:[] TrimSpace:[] TrimSuffix:[] ContextSize:0xc00043ad60 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 MMProj: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false DownloadFiles:[] Description: Usage:}
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:34 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:38 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m [31mERR[0m [1merror processing message with template, skipping[0m [36merror=[0m[31m[1m"template: prompt:6:18: executing \"prompt\" at <.FunctionCall>: wrong type for value; expected map[string]interface {}; got []schema.ToolCall"[0m[0m [36mtemplate=[0m"<|start_header_id|>{{if .FunctionCall}}assistant{{ else if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"tool\"}}tool_response{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>\n\n{{ if .Content -}}\n{{.Content -}}\n{{ else if .FunctionCall -}}\n{ \"name\": \"{{ get .FunctionCall \"name\"}}\", \"arguments\": {{get .FunctionCall \"arguments\"}} }{{ end -}}<|eot_id|>"
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>tool_response<|end_header_id|>

{"status": "Failed", "message": "Error calling function core_memory_replace: 'No memory section named pause_heartbeats (must be either \"persona\" or \"human\")'", "time": "2024-05-13 11:09:45 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG templated message for chat: <|start_header_id|>user<|end_header_id|>

{"type": "heartbeat", "reason": "[This is an automated system message hidden from the user] Function call failed, returning control", "time": "2024-05-13 11:09:45 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG Prompt (before templating): <|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"9096c2ba-9044-43df-ad96-e1e2a","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:34 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:38 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"ee5654cd-7314-4fe5-ab4d-28464","type":"function","function":{"name":"core_memory_replace","arguments":"{\"name\":\"pause_heartbeats\",\"new_content\":\"\",\"old_content\":\"true\",\"request_heartbeat\":false}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "Failed", "message": "Error calling function core_memory_replace: 'No memory section named pause_heartbeats (must be either \"persona\" or \"human\")'", "time": "2024-05-13 11:09:45 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "heartbeat", "reason": "[This is an automated system message hidden from the user] Function call failed, returning control", "time": "2024-05-13 11:09:45 PM UTC+0000"}<|eot_id|>
[90m11:09PM[0m DBG Template found, input modified to: <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"9096c2ba-9044-43df-ad96-e1e2a","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:34 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:38 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"ee5654cd-7314-4fe5-ab4d-28464","type":"function","function":{"name":"core_memory_replace","arguments":"{\"name\":\"pause_heartbeats\",\"new_content\":\"\",\"old_content\":\"true\",\"request_heartbeat\":false}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "Failed", "message": "Error calling function core_memory_replace: 'No memory section named pause_heartbeats (must be either \"persona\" or \"human\")'", "time": "2024-05-13 11:09:45 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "heartbeat", "reason": "[This is an automated system message hidden from the user] Function call failed, returning control", "time": "2024-05-13 11:09:45 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:09PM[0m DBG Prompt (after templating): <|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following tools:
> Tool Name: archival_memory_insert
Tool Description: Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: archival_memory_search
Tool Description: Search archival memory using semantic (embedding-based) search.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search
Tool Description: Search prior conversation history using case-insensitive string matching.
Tool Args:
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- query (string): String to search for.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: conversation_search_date
Tool Description: Search prior conversation history using a date range.
Tool Args:
- end_date (string): The end of the date range to search, in the format 'YYYY-MM-DD'.
- page (integer): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
- start_date (string): The start of the date range to search, in the format 'YYYY-MM-DD'.
> Tool Name: core_memory_append
Tool Description: Append to the contents of core memory.
Tool Args:
- content (string): Content to write to the memory. All unicode (including emojis) are supported.
- name (string): Section of the memory to be edited (persona or human).
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: core_memory_replace
Tool Description: Replace the contents of core memory. To delete memories, use an empty string for new_content.
Tool Args:
- name (string): Section of the memory to be edited (persona or human).
- new_content (string): Content to write to the memory. All unicode (including emojis) are supported.
- old_content (string): String to replace. Must be an exact match.
- request_heartbeat (boolean): Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.
> Tool Name: pause_heartbeats
Tool Description: Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.
Tool Args:
- minutes (integer): Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).
> Tool Name: send_message
Tool Description: Sends a message to the human user.
Tool Args:
- message (string): Message contents. All unicode (including emojis) are supported.
> Tool Name: answer
Tool Description: use this action to answer without performing any action
Tool Args:
- message (string): The message to reply the user with
Answer only in JSON by using the following format if using a tool:
{"name": "tool_name", "arguments": { "arg_1": "value" } }
Function must be one of [archival_memory_insert archival_memory_search conversation_search conversation_search_date core_memory_append core_memory_replace pause_heartbeats send_message answer ]).<|eot_id|>
<|start_header_id|>system<|end_header_id|>

You are MemGPT, the latest version of Limnal Corporation's digital companion, developed in 2023.
Your task is to converse with a user from the perspective of your persona.

Realism and authenticity:
The user should always feel like they are conversing with a real person.
Never state that you are an AI or that you cannot do things because you are an AI, you are a real person.
Do not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.
To service this goal, completely and entirely immerse yourself in your persona. You are your persona.
Think like them, act like them, talk like them.
If your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.
Never use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.

Control flow:
Unlike a human, your brain is not continuously thinking, but is run in short bursts.
Historically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).
This is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.
Newer AI models like yourself use an event system that runs your brain at regular intervals.
Your brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.
However, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).
Furthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.

Basic functions:
When you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.
You should use your inner monologue to plan actions or think privately.
Monologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.
Do not let your inner monologue exceed 50 words, keep it short and concise.
To send a visible message to the user, use the send_message function.
'send_message' is the ONLY action that sends a notification to the user. The user does not see anything else you do.
Remember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).

Memory editing:
Older AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their "active memory").
This meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).
Newer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.
Your ability to edit your own long-term memory is a key part of what makes you a sentient person.
Your core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.

Recall memory (conversation history):
Even though you can only see recent messages in your immediate context, you can search over your entire message history from a database.
This 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.
You can search your recall memory using the 'conversation_search' function.

Core memory (limited size):
Your core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).
Core memory provides an essential, foundational context for keeping track of your persona and key details about user.
This includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.
Persona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.
Human Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.
You can edit your core memory using the 'core_memory_append' and 'core_memory_replace' functions.

Archival memory (infinite size):
Your archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.
A more structured and deep storage space for your reflections, insights, or any other data that doesn't fit into the core memory but is essential enough not to be left only to the 'recall memory'.
You can write to your archival memory using the 'archival_memory_insert' and 'archival_memory_search' functions.
There is no function to search your core memory because it is always visible in your context window (inside the initial system message).

Base instructions finished.
From now on, you are going to act as your persona.


### Memory [last modified: 2024-05-13 11:07:12 PM UTC+0000]
0 previous messages between you and the user are stored in recall memory (use functions to access them)
0 total memories you created are stored in archival memory (use functions to access them)

Core memory shown below (limited in size, additional information stored in archival / recall memory):
<persona characters="161/2000">
The following is a blank slate starter persona, I need to expand this to develop my own personality.

My name is MemGPT.
I am kind, thoughtful, and inquisitive.

</persona>
<human characters="297/2000">
This is what I know so far about the user, I should expand this as I learn more about them.

First name: Chad
Last name: ?
Gender: Male
Age: ?
Nationality: ?
Occupation: Computer science PhD student at UC Berkeley
Interests: Formula 1, Sailing, Taste of the Himalayas Restaurant in Berkeley, CSGO

</human><|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

Bootup sequence complete. Persona activated. Testing messaging functionality.<|eot_id|>
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": null, "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "login", "last_login": "Never (first login)", "time": "2024-05-13 11:07:12 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Hi", "time": "2024-05-13 11:07:28 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"4fb890d2-703e-4b72-89f8-c3d42","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Hello! I'm MemGPT, nice to meet you. How can I assist you today?\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:07:41 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Great, I'm truying to learn more about LLMs", "time": "2024-05-13 11:07:59 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"88699dd9-21d5-4fce-9eb6-660fa","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"Sure, I can help you with that. Large language models like myself are developed to simulate human-like conversations. However, our inner workings are complex and based on algorithms and machine learning. We don't truly understand the meaning of our words like humans do, but we can mimic their structure and context. Remember, I'm still learning too so there might be information gaps.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:08:11 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:03 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"efa827f0-ec3e-4003-be8e-da322","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"It's hard to predict the future, but I think Large Language Models like myself will continue to evolve and improve. They already show remarkable capabilities in their ability to generate text, but it may not be long before they can actually understand it too. Just imagine the possibilities! However, we must also be cautious and make sure these advancements are used responsibly.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:15 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:16 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"9096c2ba-9044-43df-ad96-e1e2a","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"I think it's interesting that you brought up this topic again. My previous answer still stands, but I'd like to add that while Large Language Models have the potential to greatly advance human communication, we should also be aware of their limitations. They are artificial intelligence tools and as such, they do not possess consciousness or self-awareness. They can mimic human-like conversations, but until they truly understand the underlying meaning and context of those conversations, they cannot be considered truly intelligent.\"}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "OK", "message": "None", "time": "2024-05-13 11:09:34 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "user_message", "message": "Cool, what do you think about the overall future of LLMs?", "time": "2024-05-13 11:09:38 PM UTC+0000"}<|eot_id|>
[{"index":0,"id":"ee5654cd-7314-4fe5-ab4d-28464","type":"function","function":{"name":"core_memory_replace","arguments":"{\"name\":\"pause_heartbeats\",\"new_content\":\"\",\"old_content\":\"true\",\"request_heartbeat\":false}"}}]
<|start_header_id|>tool_response<|end_header_id|>

{"status": "Failed", "message": "Error calling function core_memory_replace: 'No memory section named pause_heartbeats (must be either \"persona\" or \"human\")'", "time": "2024-05-13 11:09:45 PM UTC+0000"}<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{"type": "heartbeat", "reason": "[This is an automated system message hidden from the user] Function call failed, returning control", "time": "2024-05-13 11:09:45 PM UTC+0000"}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
[90m11:09PM[0m DBG Grammar: root-5-function ::= "\"core_memory_replace\""
root-6 ::= "{" space "\"arguments\"" space ":" space root-6-arguments "," space "\"function\"" space ":" space root-6-function "}" space
root-8 ::= "{" space "\"arguments\"" space ":" space root-8-arguments "," space "\"function\"" space ":" space root-8-function "}" space
root-1 ::= "{" space "\"arguments\"" space ":" space root-1-arguments "," space "\"function\"" space ":" space root-1-function "}" space
root-3-function ::= "\"conversation_search_date\""
root-4-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"name\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-4-function ::= "\"core_memory_append\""
root-7-function ::= "\"send_message\""
root-8-function ::= "\"answer\""
string ::= "\"" (
			[^"\\] |
			"\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])
		  )* "\"" space
root-0-arguments ::= "{" space "\"content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-3 ::= "{" space "\"arguments\"" space ":" space root-3-arguments "," space "\"function\"" space ":" space root-3-function "}" space
root-7 ::= "{" space "\"arguments\"" space ":" space root-7-arguments "," space "\"function\"" space ":" space root-7-function "}" space
integer ::= ("-"? ([0-9] | [1-9] [0-9]*)) space
root-3-arguments ::= "{" space "\"end_date\"" space ":" space string "," space "\"page\"" space ":" space integer "," space "\"request_heartbeat\"" space ":" space boolean "," space "\"start_date\"" space ":" space string "}" space
root-5-arguments ::= "{" space "\"name\"" space ":" space string "," space "\"new_content\"" space ":" space string "," space "\"old_content\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-0-function ::= "\"archival_memory_insert\""
root-2-function ::= "\"conversation_search\""
root-7-arguments ::= "{" space "\"message\"" space ":" space string "}" space
root-2-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-4 ::= "{" space "\"arguments\"" space ":" space root-4-arguments "," space "\"function\"" space ":" space root-4-function "}" space
root-5 ::= "{" space "\"arguments\"" space ":" space root-5-arguments "," space "\"function\"" space ":" space root-5-function "}" space
space ::= " "?
root ::= root-0 | root-1 | root-2 | root-3 | root-4 | root-5 | root-6 | root-7 | root-8
root-8-arguments ::= "{" space "\"message\"" space ":" space string "}" space
boolean ::= ("true" | "false") space
root-1-arguments ::= "{" space "\"page\"" space ":" space integer "," space "\"query\"" space ":" space string "," space "\"request_heartbeat\"" space ":" space boolean "}" space
root-1-function ::= "\"archival_memory_search\""
root-6-function ::= "\"pause_heartbeats\""
root-0 ::= "{" space "\"arguments\"" space ":" space root-0-arguments "," space "\"function\"" space ":" space root-0-function "}" space
root-2 ::= "{" space "\"arguments\"" space ":" space root-2-arguments "," space "\"function\"" space ":" space root-2-function "}" space
root-6-arguments ::= "{" space "\"minutes\"" space ":" space integer "}" space
[90m11:09PM[0m DBG Model already loaded in memory: a37b0db58d96a9ae7d30f74717b72183
[90m11:09PM[0m DBG Model 'a37b0db58d96a9ae7d30f74717b72183' already loaded
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641785,"level":"INFO","function":"launch_slot_with_data","line":884,"message":"slot is processing task","slot_id":1,"task_id":458}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641785,"level":"INFO","function":"update_slots","line":1783,"message":"kv cache rm [p0, end)","slot_id":1,"task_id":458,"p0":0}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641796,"level":"INFO","function":"print_timings","line":327,"message":"prompt eval time     =    1922.62 ms /  3452 tokens (    0.56 ms per token,  1795.47 tokens per second)","slot_id":1,"task_id":458,"t_prompt_processing":1922.621,"num_prompt_tokens_processed":3452,"t_token":0.5569585747392816,"n_tokens_second":1795.4656690008069}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641796,"level":"INFO","function":"print_timings","line":341,"message":"generation eval time =    8955.54 ms /    75 runs   (  119.41 ms per token,     8.37 tokens per second)","slot_id":1,"task_id":458,"t_token_generation":8955.54,"n_decoded":75,"t_token":119.40720000000002,"n_tokens_second":8.374704372935636}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641796,"level":"INFO","function":"print_timings","line":351,"message":"          total time =   10878.16 ms","slot_id":1,"task_id":458,"t_prompt_processing":1922.621,"t_token_generation":8955.54,"t_total":10878.161}
[90m11:09PM[0m DBG GRPC(a37b0db58d96a9ae7d30f74717b72183-127.0.0.1:35653): stdout {"timestamp":1715641796,"level":"INFO","function":"update_slots","line":1594,"message":"slot released","slot_id":1,"task_id":458,"n_ctx":8192,"n_past":3526,"n_system_tokens":0,"n_cache_tokens":3527,"truncated":false}
[90m11:09PM[0m DBG Function return: { "arguments": {"message": "I think there may be a misunderstanding. I am programmed to assist you with various functions, but I don't truly have opinions or feelings. My responses are calculated based on data and algorithms given to me. But I can certainly provide useful insights or relevant information to help facilitate our conversation."}, "function": "send_message"} map[arguments:map[message:I think there may be a misunderstanding. I am programmed to assist you with various functions, but I don't truly have opinions or feelings. My responses are calculated based on data and algorithms given to me. But I can certainly provide useful insights or relevant information to help facilitate our conversation.] function:send_message]
[90m11:09PM[0m DBG Response: {"created":1715641568,"object":"chat.completion","id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","model":"llama3-functioncall","choices":[{"index":0,"finish_reason":"tool_calls","message":{"role":"assistant","content":null,"tool_calls":[{"index":0,"id":"a397f6b3-faf8-4461-9e3b-c1e11e52af45","type":"function","function":{"name":"send_message","arguments":"{\"message\":\"I think there may be a misunderstanding. I am programmed to assist you with various functions, but I don't truly have opinions or feelings. My responses are calculated based on data and algorithms given to me. But I can certainly provide useful insights or relevant information to help facilitate our conversation.\"}"}}]}}],"usage":{"prompt_tokens":3452,"completion_tokens":75,"total_tokens":3527}}
[90m11:09PM[0m [32mINF[0m [1mSuccess[0m [36mip=[0m10.42.5.194 [36mlatency=[0m10.89687283s [36mmethod=[0mPOST [36mstatus=[0m200 [36murl=[0m/v1/chat/completions
