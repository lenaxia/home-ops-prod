
```

 ┌───────────────────────────────────────────────────┐
 │                   Fiber v2.46.0                   │
 │               http://127.0.0.1:8080               │
 │       (bound on host 0.0.0.0 and port 8080)       │
 │                                                   │
 │ Handlers ............ 21  Processes ........... 1 │
 │ Prefork ....... Disabled  PID .............. 6219 │
 └───────────────────────────────────────────────────┘

[10.42.1.0]:10528  200  -  GET      /models/
9:12PM DBG Request received: {"model":"ggml-gpt4all-j","file":"","language":"","response_format":"","size":"","prompt":null,"instruction":"","input":null,"stop":null,"messages":[{"role":"user","content":"How are you?"}],"stream":false,"echo":false,"top_p":0,"top_k":0,"temperature":0.9,"max_tokens":0,"n":0,"batch":0,"f16":false,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"mirostat_eta":0,"mirostat_tau":0,"mirostat":0,"seed":0,"mode":0,"step":0}
9:12PM DBG Parameter Config: &{OpenAIRequest:{Model:ggml-gpt4all-j File: Language: ResponseFormat: Size: Prompt:<nil> Instruction: Input:<nil> Stop:<nil> Messages:[] Stream:false Echo:false TopP:0.7 TopK:80 Temperature:0.9 Maxtokens:512 N:0 Batch:0 F16:false IgnoreEOS:false RepeatPenalty:0 Keep:0 MirostatETA:0 MirostatTAU:0 Mirostat:0 Seed:0 Mode:0 Step:0} Name: StopWords:[] Cutstrings:[] TrimSpace:[] ContextSize:1024 F16:false Threads:8 Debug:true Roles:map[] Embeddings:false Backend: TemplateConfig:{Completion: Chat: Edit:} MirostatETA:0 MirostatTAU:0 Mirostat:0 NGPULayers:0 ImageGenerationAssets: PromptStrings:[] InputStrings:[] InputToken:[]}
9:12PM DBG Loading model 'ggml-gpt4all-j' greedly
9:12PM DBG [llama] Attempting to load
9:12PM DBG Loading model llama from ggml-gpt4all-j
9:12PM DBG Loading model in memory from file: /models/ggml-gpt4all-j
llama.cpp: loading model from /models/ggml-gpt4all-j
failed unexpectedly reached end of file9:12PM DBG [llama] Fails: failed loading model
9:12PM DBG [gpt4all-llama] Attempting to load
9:12PM DBG Loading model gpt4all-llama from ggml-gpt4all-j
9:12PM DBG Loading model in memory from file: /models/ggml-gpt4all-j
llama.cpp: loading model from /models/ggml-gpt4all-j
error loading model: unexpectedly reached end of file
gptjllama_init_from_file: failed to load model
LLAMA ERROR: failed to load model from /models/ggml-gpt4all-j
9:12PM DBG [gpt4all-llama] Fails: failed loading model
9:12PM DBG [gpt4all-mpt] Attempting to load
9:12PM DBG Loading model gpt4all-mpt from ggml-gpt4all-j
9:12PM DBG Loading model in memory from file: /models/ggml-gpt4all-j
mpt_model_load: loading model from '/models/ggml-gpt4all-j' - please wait ...
mpt_model_load: invalid model file '/models/ggml-gpt4all-j' (bad magic)
GPT-J ERROR: failed to load model from /models/ggml-gpt4all-j9:12PM DBG [gpt4all-mpt] Fails: failed loading model
9:12PM DBG [gpt4all-j] Attempting to load
9:12PM DBG Loading model gpt4all-j from ggml-gpt4all-j
9:12PM DBG Loading model in memory from file: /models/ggml-gpt4all-j
gptj_model_load: loading model from '/models/ggml-gpt4all-j' - please wait ...
gptj_model_load: n_vocab = 50400
gptj_model_load: n_ctx   = 2048
gptj_model_load: n_embd  = 4096
gptj_model_load: n_head  = 16
gptj_model_load: n_layer = 28
gptj_model_load: n_rot   = 64
gptj_model_load: f16     = 2
gptj_model_load: ggml ctx size = 5401.45 MB
gptj_model_load: kv self size  =  896.00 MB
gptj_model_load: ................................... done
gptj_model_load: model size =  3609.38 MB / num tensors = 285
9:12PM DBG [gpt4all-j] Loads OK
9:12PM DBG Response: {"object":"chat.completion","model":"ggml-gpt4all-j","choices":[{"message":{"role":"assistant","content":"I'm fine, thank you. How about yourself?\nI'm doing well, thank you. How about yourself?"}}],"usage":{"prompt_tokens":0,"completion_tokens":0,"total_tokens":0}}
[10.42.1.0]:25765  200  -  POST     /v1/chat/completions
9:13PM DBG Request received: {"model":"","file":"","language":"","response_format":"","size":"256x256","prompt":"A cute baby sea otter","instruction":"","input":null,"stop":null,"messages":null,"stream":false,"echo":false,"top_p":0,"top_k":0,"temperature":0,"max_tokens":0,"n":0,"batch":0,"f16":false,"ignore_eos":false,"repeat_penalty":0,"n_keep":0,"mirostat_eta":0,"mirostat_tau":0,"mirostat":0,"seed":0,"mode":0,"step":0}
9:13PM DBG Loading model: stablediffusion
9:13PM DBG Parameter Config: &{OpenAIRequest:{Model: File: Language: ResponseFormat: Size: Prompt:<nil> Instruction: Input:<nil> Stop:<nil> Messages:[] Stream:false Echo:false TopP:0 TopK:0 Temperature:0 Maxtokens:0 N:0 Batch:0 F16:false IgnoreEOS:false RepeatPenalty:0 Keep:0 MirostatETA:0 MirostatTAU:0 Mirostat:0 Seed:0 Mode:0 Step:0} Name:stablediffusion StopWords:[] Cutstrings:[] TrimSpace:[] ContextSize:0 F16:false Threads:4 Debug:true Roles:map[] Embeddings:false Backend:stablediffusion TemplateConfig:{Completion: Chat: Edit:} MirostatETA:0 MirostatTAU:0 Mirostat:0 NGPULayers:0 ImageGenerationAssets:stablediffusion_assets PromptStrings:[A cute baby sea otter] InputStrings:[] InputToken:[]}
9:13PM DBG Loading model stablediffusion from stablediffusion_assets
9:13PM DBG Loading model in memory from file: /models/stablediffusion_assets
----------------[start stable diffusion]--------------------
----------------[init]--------------------
fopen /models/stablediffusion_assets/FrozenCLIPEmbedder-fp16.bin failed
fatal error: unexpected signal during runtime execution
/build/entrypoint.sh: line 9:  6219 Segmentation fault      (core dumped) ./local-ai "$@"
```

