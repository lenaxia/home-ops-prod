# yaml-language-server: $schema=https://kubernetes-schemas.devbu.io/helm.toolkit.fluxcd.io/helmrelease_v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &appname litellm
  namespace: &namespace home
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 3.1.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s-charts
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    controllers:
      main:
        type: statefulset
        annotations:
          reloader.stakater.com/auto: 'true'
        containers:
          main:
            image:
              repository: ghcr.io/berriai/litellm
              tag: main-latest
            args:
              - "--config"
              - "/app/proxy_server_config.yaml"
            env:
              TZ: ${TIMEZONE}
            envFrom:
              - secretRef:
                  name: *appname 

            resources:
              requests:
                cpu: 15m
                memory: 94M

    configMaps:
      configyaml:
        enabled: true
        data:
          config.yaml: |
              model_list: 
                # LocalAI
                - model_name: localai-hermes2pro-llama3
                  litellm_params:
                    model: openai/hermes-2-pro-theta-llama3
                    api_base: http://localai.home.svc.cluster.local:8080
                    api_key: doesnotmatter

                # Ollama
                - model_name: ollama-openhermes
                  litellm_params:
                    model: ollama_chat/openhermes:7b-mistral-v2.5-q8_0
                    api_base: http://ollama.home.svc.cluster.local:11434
                - model_name: ollama-llama3
                  litellm_params:
                    model: ollama_chat/llama3:8b-instruct-q8_0
                    api_base: http://ollama.home.svc.cluster.local:11434
                - model_name: ollama-phi3
                  litellm_params:
                    model: ollama_chat/phi3:latest
                    api_base: http://ollama.home.svc.cluster.local:11434

                # Bedrock
                - model_name: bedrock-claude-v2-sonnet
                  litellm_params:
                    model: bedrock/anthropic.claude-3-sonnet-20240229-v1:0
                    aws_region_name: us-west-2
                    drop_params: "true"
                - model_name: bedrock-claude-v2-haiku
                  litellm_params:
                    model: bedrock/anthropic.claude-3-haiku-20240307-v1:0
                    aws_region_name: us-west-2
                    drop_params: "true"
                - model_name: bedrock-claude-v2-opus
                  litellm_params:
                    model: bedrock/anthropic.claude-3-opus-20240229-v1:0
                    aws_region_name: us-west-2
                    drop_params: "true"
                - model_name: bedrock-mixtral-8x7b
                  litellm_params:
                    model: bedrock/mistral.mixtral-8x7b-instruct-v0:1
                    aws_region_name: us-west-2
                    drop_params: "true"
                - model_name: bedrock-llama3-70b
                  litellm_params:
                    model: bedrock/meta.llama3-70b-instruct-v1:0
                    aws_region_name: us-west-2
                    drop_params: "true"
                - model_name: bedrock-command-r-plus
                  litellm_params:
                    model: bedrock/cohere.command-r-plus-v1:0
                    aws_region_name: us-west-2
                    drop_params: "true"
    persistence:
      configyaml:
        enabled: true
        type: configMap
        name: litellm-configyaml
        globalMounts:
          - path: /app/proxy_server_config.yaml
            subPath: config.yaml
    service:
      main:
        ports:
          http:
            port: 4000
        primary: true
        controller: main
        type: LoadBalancer
    ingress:
      main:
        enabled: true
        className: traefik
        annotations:
          hajimari.io/enable: 'true'
          hajimari.io/icon: baby-bottle-outline
          hajimari.io/info: ChatGPT
          hajimari.io/group: &namespace home
          cert-manager.io/cluster-issuer: letsencrypt-production
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
          traefik.ingress.kubernetes.io/router.middlewares: networking-chain-authelia@kubernetescrd
        hosts:
          - host: &uri llm.${SECRET_DEV_DOMAIN}
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: main
                  port: http
        tls:
          - hosts:
              - *uri
            secretName: *uri
    # podSecurityContext:
    #   runAsUser: 1000
    #   runAsGroup: 1000
    #   fsGroup: 1000
    #   fsGroupChangePolicy: "OnRootMismatch"
